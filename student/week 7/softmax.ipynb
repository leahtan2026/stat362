{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0b949967",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Quiz10_2: Softmax\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    code-fold: show\n",
    "    self-contained: true\n",
    "    html-math-method: mathml \n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ba48a4-f0c6-459b-8438-2423e280fd50",
   "metadata": {},
   "source": [
    "## Overview: Multiclass classification\n",
    "- A classification task with more than two classes; e.g., classify a set of images of fruits which may be oranges, apples, or pears. Multiclass classification assumes each sample is assigned to one and only one label (mutually exclusive).\n",
    "- Common examples include image classification (cat, dog, human, etc.) or handwritten digit recognition (classifying an image into a digit from 0 to 9).\n",
    "- In machine learning, multiclass (multinomial) classification is the problem of classifying instances into one of three or more classes (two classes is binary classification).\n",
    "- Multiclass classification should not be confused with multi-label classification, where multiple labels can be predicted for each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8878071f-327e-4735-9428-14ba3806ec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib as mpl\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c392d9-398d-4852-809e-3f4c3c5b5681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some helper function below to plot data\n",
    "def plt_mc_data(ax, X, y, classes,  class_labels=None, map=plt.cm.Paired, \n",
    "                legend=False, size=50, m='o', equal_xy = False):\n",
    "    \"\"\" Plot multiclass data. Note, if equal_xy is True, setting ylim on the plot may not work \"\"\"\n",
    "    \n",
    "    for i in range(classes):\n",
    "        idx = np.where(y == i)\n",
    "        col = len(idx[0])*[i]\n",
    "        label = class_labels[i] if class_labels else \"c{}\".format(i)\n",
    "        # Use numeric class labels with a colormap to avoid warnings about vmin/vmax being ignored\n",
    "        ax.scatter(X[idx, 0], X[idx, 1],  marker=m,\n",
    "                    c=col, cmap=map, vmin=0, vmax=classes-1,\n",
    "                    s=size, label=label)\n",
    "    if legend: ax.legend()\n",
    "    if equal_xy: ax.axis(\"equal\")\n",
    "\n",
    "def plt_mc(X_train,y_train,classes, centers, std):\n",
    "    css = np.unique(y_train)\n",
    "    fig,ax = plt.subplots(1,1,figsize=(3,3))\n",
    "    fig.canvas.toolbar_visible = False\n",
    "    fig.canvas.header_visible = False\n",
    "    fig.canvas.footer_visible = False\n",
    "    plt_mc_data(ax, X_train,y_train,classes, map=plt.cm.Paired, legend=True, size=50, equal_xy = False)\n",
    "    ax.set_title(\"Multiclass Data\")\n",
    "    ax.set_xlabel(\"x0\")\n",
    "    ax.set_ylabel(\"x1\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891a71fe",
   "metadata": {},
   "source": [
    "## 1. Prepare and visualize our data\n",
    "We will use Scikit-Learn `make_blobs` function to make a training data set with 4 categories as shown in the plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcb67bf-a232-46d1-9da5-f1b1d4965450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make 4-class dataset for classification\n",
    "classes = 4\n",
    "m = 500\n",
    "centers = [[-5, 2], [-2, -2], [1, 2], [5, -2]]\n",
    "std = 1.0\n",
    "X, y = make_blobs(n_samples=m, centers=centers, cluster_std=std,random_state=30)\n",
    "\n",
    "# add more noise to the data\n",
    "np.random.seed(42)\n",
    "X += np.random.normal(0, 0.75, X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0996f377-d18f-4564-9c79-1e50bd2e8bc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt_mc(X,y,classes, centers, std=std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11371fe5-faca-4f5b-b4ac-7b753a5dc84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# plot side by side subplots on training and test data\n",
    "fig,ax = plt.subplots(1,2,figsize=(6,3))\n",
    "fig.canvas.toolbar_visible = False\n",
    "fig.canvas.header_visible = False\n",
    "fig.canvas.footer_visible = False\n",
    "plt_mc_data(ax[0], X_train,y_train,classes, map=plt.cm.Paired, legend=True, size=50, equal_xy = False)\n",
    "ax[0].set_title(\"Training Data\")\n",
    "ax[0].set_xlabel(\"x0\")\n",
    "ax[0].set_ylabel(\"x1\")\n",
    "plt_mc_data(ax[1], X_test,y_test,classes, map=plt.cm.Paired, legend=True, size=50, equal_xy = False)\n",
    "ax[1].set_title(\"Test Data\")\n",
    "ax[1].set_xlabel(\"x0\")\n",
    "ax[1].set_ylabel(\"x1\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab8e5ac",
   "metadata": {},
   "source": [
    "##  Softmax (Multinominal Logistic Regression)\n",
    "\n",
    "###  Softmax Function\n",
    "\n",
    "In softmax regression, N outputs are generated and one output is selected as the predicted category. In this case a vector $\\mathbf{z}$ is generated by a linear function which is applied to a softmax function. The softmax function converts $\\mathbf{z}$  into a probability distribution as described below. After applying softmax, each output will be between 0 and 1 and the outputs will add to 1, so that they can be interpreted as probabilities. The larger inputs  will correspond to larger output probabilities.\n",
    "<center>  <img  src=\"./images/C2_W2_SoftmaxReg_NN.png\" width=\"600\" />  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1293765",
   "metadata": {},
   "source": [
    "The softmax function can be written:\n",
    "\n",
    "$$a_j = \\frac{e^{z_j}}{ \\sum_{k=1}^{N}{e^{z_k} }} \\tag{1}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04414803",
   "metadata": {},
   "source": [
    "Which shows the output is a vector of probabilities. The first entry is the probability the input is the first category given the input $\\mathbf{x}$ and parameters $\\mathbf{w}$ and $\\mathbf{b}$.  \n",
    "Let's create a NumPy implementation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed563dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softmax(z):\n",
    "    ez = np.exp(z)              #element-wise exponenial\n",
    "    sm = ez/np.sum(ez)\n",
    "    return(sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b566ab",
   "metadata": {},
   "source": [
    "###  Softmax Cost\n",
    "<center> <img  src=\"./images/C2_W2_SoftMaxCost.png\" width=\"400\" />    <center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357ebe6a",
   "metadata": {},
   "source": [
    "The loss function associated with Softmax, the cross-entropy loss, is:\n",
    "\\begin{equation}\n",
    "  L(\\mathbf{a},y)=\\begin{cases}\n",
    "    -log(a_1), & \\text{if $y=1$}.\\\\\n",
    "        &\\vdots\\\\\n",
    "     -log(a_N), & \\text{if $y=N$}\n",
    "  \\end{cases} \\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "Where y is the target category for this example and $\\mathbf{a}$ is the output of a softmax function. In particular, the values in $\\mathbf{a}$ are probabilities that sum to one.\n",
    ">**Recall:** In this course, Loss is for one example while Cost covers all examples. \n",
    " \n",
    " \n",
    "Note in (3) above, only the line that corresponds to the target contributes to the loss, other lines are zero. To write the cost equation we need an 'indicator function' that will be 1 when the index matches the target and zero otherwise. \n",
    "    $$\\mathbf{1}\\{y == n\\} = =\\begin{cases}\n",
    "    1, & \\text{if $y==n$}.\\\\\n",
    "    0, & \\text{otherwise}.\n",
    "  \\end{cases}$$\n",
    "Now the cost is:\n",
    "\\begin{align}\n",
    "J(\\mathbf{w},b) = -\\frac{1}{m} \\left[ \\sum_{i=1}^{m} \\sum_{j=1}^{N}  1\\left\\{y^{(i)} == j\\right\\} \\log \\frac{e^{z^{(i)}_j}}{\\sum_{k=1}^N e^{z^{(i)}_k} }\\right] \\tag{4}\n",
    "\\end{align}\n",
    "\n",
    "Where $m$ is the number of examples, $N$ is the number of outputs. This is the average of all the losses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4da904e",
   "metadata": {},
   "source": [
    "\n",
    "###  Multinominal Logistic Regression\n",
    "\n",
    "Let's build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b70087-6c32-4945-b980-7b2998dae39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# Train multinomial logistic regression classifier\n",
    "logreg_classifier = LogisticRegression()\n",
    "logreg_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Get the predicted probabilities for each class\n",
    "y_pred_probabilities = logreg_classifier.predict_proba(X_test)\n",
    "y_pred_probabilities[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131adeb1-c48d-4aa6-b4ec-6972343070ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg_classifier.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1abbdb-f952-4ed4-bc57-7e9fa58a066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "pd.DataFrame(confusion_matrix(y_test,y_pred))\n",
    "print(round(accuracy_score(y_test, y_pred), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e6f735-1f23-470b-ba23-40a6a944555e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aea43dd-f4a4-44cb-9ba2-0f6c25e3565d",
   "metadata": {},
   "source": [
    "###  Softmax Classifier in Neural Nets\n",
    "\n",
    "\n",
    "#### MLPClassifier from Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd4282d",
   "metadata": {},
   "source": [
    "\n",
    "`MLPClassifier` in scikit-learn is a multi-layer perceptron (MLP) classifier, which is a type of artificial neural network. It can be used for multi-class classification tasks by learning a mapping from input features to output class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842caafb-e110-4ff0-8d5e-6049cb323a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a neural network classifier on the data\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000)\n",
    "mlp_classifier.fit(X_train, y_train)\n",
    "\n",
    "# make prediction on the test data\n",
    "y_pred = mlp_classifier.predict(X_test)\n",
    "\n",
    "# output the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# output the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6806fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = mlp_classifier.predict_proba(X_test)\n",
    "print(\"Prediction Probabilities:\", y_pred_prob[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8367217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add all probabities for each instance \n",
    "y_pred_prob.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c52ca95",
   "metadata": {},
   "source": [
    "Noticeably, the sum of probabilities for each instance totals to 1, a characteristic indicative of the use of softmax for multi-class classification. Now, let's take a step further and explicitly construct a neural network classifier, specifying the output layer with softmax activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54336ec",
   "metadata": {},
   "source": [
    "####  Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad707b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set PyTorch as the backend for Keras before importing Keras\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "\n",
    "# Now import Keras (it will use PyTorch as backend)\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# build a neural network classifier on the data\n",
    "keras_classifier = Sequential(\n",
    "    [ \n",
    "        Dense(25, activation = 'relu'),\n",
    "        Dense(15, activation = 'relu'),\n",
    "        Dense(4, activation = 'softmax')    # < softmax activation here\n",
    "    ]\n",
    ")\n",
    "\n",
    "keras_classifier.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "keras_classifier.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1afdac",
   "metadata": {},
   "source": [
    "Because the softmax is integrated into the output layer, the output is a vector of probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9397625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show first few predicted probability vectors from the Keras (torch-backend) model\n",
    "p_nonpreferred = keras_classifier.predict(X_test)\n",
    "print(p_nonpreferred[:2])\n",
    "print(\"largest value\", np.max(p_nonpreferred), \"smallest value\", np.min(p_nonpreferred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a374810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output accuracy and confusion matrix\n",
    "y_pred = np.argmax(p_nonpreferred, axis=1)\n",
    "print(round(accuracy_score(y_test, y_pred), 2))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94cb52a",
   "metadata": {},
   "source": [
    "##  Accuracy, precision, recall, and f1 score for multiclass classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41de1d76",
   "metadata": {},
   "source": [
    "In multiclass classification, accuracy is calculated in the same way as in binary classification, but precision, recall, and F1-score are calculated for each class individually.\n",
    "\n",
    "Please refer to the following figure for the caculation\n",
    "\n",
    "<img src=\"https://www.sefidian.com/wp-content/uploads/2023/04/image-40.png\" alt=\"Image\" style=\"width:450px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1df7837",
   "metadata": {},
   "outputs": [],
   "source": [
    "## output the performance metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add18618",
   "metadata": {},
   "source": [
    "In multiclass classification, these metrics are calculated separately for each class, and then typically aggregated using some form of averaging (e.g., micro-average, macro-average, weighted average) to obtain overall performance measures. This allows us to evaluate the classifier's performance on each class individually as well as on the entire dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c11fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## output the precision, recall and f1 score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"Precision: {:.2f}\".format(precision_score(y_test, y_pred, average='macro')))\n",
    "print(\"Recall: {:.2f}\".format(recall_score(y_test, y_pred, average='macro')))\n",
    "print(\"F1 Score: {:.2f}\".format(f1_score(y_test, y_pred, average='macro')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66610cd7",
   "metadata": {},
   "source": [
    "In multiclass classification, macro-average and weighted average are two commonly used methods for aggregating performance metrics (such as precision, recall, and F1-score) across multiple classes to obtain overall performance measures.\n",
    "\n",
    "Below is an explanation of each:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cffb3e3",
   "metadata": {},
   "source": [
    "### Macro-averaging\n",
    "**verage the precision and recall across all classes** to get the final macro-averaged precision and recall scores.\n",
    "\n",
    "<img src=\"https://assets-global.website-files.com/6266b596eef18c1931f938f9/644afed8b72fe836c6eae060_class_guide_multi_abc13.png\" alt=\"Image\" style=\"width:450px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e1a96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## output the precision, recall and f1 score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"Precision: {:.2f}\".format(precision_score(y_test, y_pred, average='weighted')))\n",
    "print(\"Recall: {:.2f}\".format(recall_score(y_test, y_pred, average='weighted')))\n",
    "print(\"F1 Score: {:.2f}\".format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402b2666",
   "metadata": {},
   "source": [
    "### Weighted averaging\n",
    "This approach takes into account the balance of classes. You weigh each class based on its representation in the dataset. Then, you compute precision and recall as a weighted average of the precision and recall in individual classes.\n",
    "\n",
    "Simply put, it would work like macro-averaging, but instead of dividing precision and recall by the number of classes, you give each class a fair representation based on the proportion it takes in the dataset. \n",
    "\n",
    "This approach is useful if you have an imbalanced dataset but want to assign larger importance to classes with more examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0462f2-86ed-41da-9659-26190edd1384",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "In this lab you \n",
    "* Extended the binary classification to multiclass classification\n",
    "* Explored different ML classifiers for multiclass classification\n",
    "* Got to know softmax function, which is widely used in neural networks\n",
    "* Explored the distinctions in performance metrics between multiclass and binary classification.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat362-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
