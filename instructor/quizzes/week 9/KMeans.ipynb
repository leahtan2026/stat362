{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Quiz 6:  Clustering Essentials ‚Äì K-Means and Beyond\"\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    code-fold: show\n",
    "    self-contained: true\n",
    "    number-sections: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to this comprehensive tutorial on **K-means clustering**! This hands-on lab will guide you through understanding, implementing, and applying one of the most fundamental unsupervised learning algorithms.\n",
    "\n",
    "\n",
    "##  Lab Structure\n",
    "\n",
    "This tutorial is organized into progressive sections:\n",
    "\n",
    "1. **Implementation Phase** - Build K-means from scratch with step-by-step guidance\n",
    "2. **Visualization & Analysis** - See the algorithm in action with interactive plots\n",
    "3. **Parameter Tuning** - Learn to find the optimal number of clusters\n",
    "4. **Scaling & Preprocessing** - Understand the importance of feature scaling\n",
    "5. **Limitations & Alternatives** - Explore when K-means fails and what to use instead\n",
    "6. **Evaluation Metrics** - Learn to assess clustering quality without ground truth\n",
    "\n",
    "Let's dive in and start clustering! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Libraries & Setup\n",
    "\n",
    "Before we dive into K-means clustering, let's import all the essential libraries we'll need throughout this tutorial. **Run the cell below** to set up your environment:\n",
    "\n",
    "- **`utils.py`** - Contains custom helper functions for visualization and data loading specific to this assignment. You don't need to modify this file.\n",
    "\n",
    "üí° **Tip:** If you encounter any import errors, make sure all required packages are installed in your environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.datasets import make_blobs, make_circles\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Introduction to K-Means Clustering\n",
    "\n",
    "### What is K-Means Clustering?\n",
    "\n",
    "K-means is an **unsupervised learning algorithm** designed to automatically discover hidden patterns in data by grouping similar data points into clusters. It's one of the most widely used clustering algorithms in machine learning and data science.\n",
    "\n",
    "#### The Problem Setup\n",
    "\n",
    "* **Input**: A training set $\\{x^{(1)}, x^{(2)}, ..., x^{(m)}\\}$ where each $x^{(i)} \\in \\mathbb{R}^n$\n",
    "* **Goal**: Partition the data into $K$ distinct, cohesive clusters\n",
    "* **Output**: Cluster assignments and centroids that minimize within-cluster variance\n",
    "\n",
    "### How K-Means Works: The Algorithm\n",
    "\n",
    "K-means follows an **iterative optimization approach** with two main phases:\n",
    "\n",
    "#### **Phase 1: Cluster Assignment**\n",
    "Assign each data point to the nearest centroid\n",
    "\n",
    "#### **Phase 2: Centroid Update** \n",
    "Move centroids to the center of their assigned points\n",
    "\n",
    "\n",
    "\n",
    "#### Algorithm Pseudocode\n",
    "\n",
    "```python\n",
    "# Step 1: Initialize centroids\n",
    "centroids = kMeans_init_centroids(X, K)\n",
    "\n",
    "# Step 2: Iterate until convergence\n",
    "for iteration in range(max_iterations):\n",
    "    # Phase 1: Assign points to closest centroids\n",
    "    idx = find_closest_centroids(X, centroids)\n",
    "    \n",
    "    # Phase 2: Update centroids to cluster means\n",
    "    centroids = compute_centroids(X, idx, K)\n",
    "```\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "#### **Objective Function (Cost Function)**\n",
    "K-means minimizes the **Within-Cluster Sum of Squares (WCSS)**:\n",
    "\n",
    "$$J = \\sum_{i=1}^{m} ||x^{(i)} - \\mu_{c^{(i)}}||^2$$\n",
    "\n",
    "Where:\n",
    "- $x^{(i)}$ = $i$-th data point\n",
    "- $\\mu_{c^{(i)}}$ = centroid of cluster assigned to $x^{(i)}$\n",
    "- $c^{(i)}$ = cluster assignment for point $i$\n",
    "\n",
    "#### **The Two-Step Process**\n",
    "\n",
    "1. **Assignment Step**: For each point $x^{(i)}$, find the closest centroid:\n",
    "   $$c^{(i)} = \\arg\\min_j ||x^{(i)} - \\mu_j||^2$$\n",
    "\n",
    "2. **Update Step**: For each centroid $k$, compute the mean of assigned points:\n",
    "   $$\\mu_k = \\frac{1}{|C_k|} \\sum_{i \\in C_k} x^{(i)}$$\n",
    "   \n",
    "   Where $C_k = \\{i : c^{(i)} = k\\}$ (set of points assigned to cluster $k$)\n",
    "\n",
    "\n",
    "### Implementation Plan\n",
    "\n",
    "In the following sections, you'll implement the two core functions:\n",
    "\n",
    "1. **`find_closest_centroids()`** - Implements the assignment step\n",
    "2. **`compute_centroids()`** - Implements the update step\n",
    "\n",
    "üí° **Pro Tip**: Understanding these two functions deeply will give you complete mastery over how K-means works under the hood!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing K-Means from Scratch\n",
    "\n",
    "### Finding Closest Centroids\n",
    "\n",
    "In the ‚Äúcluster assignment‚Äù phase of the K-means algorithm, the\n",
    "algorithm assigns every training example $x^{(i)}$ to its closest\n",
    "centroid, given the current positions of centroids. \n",
    "\n",
    "\n",
    "### **Task 1:**\n",
    "\n",
    "Your task is to complete the code in `find_closest_centroids`. \n",
    "\n",
    "* This function takes the data matrix `X` and the locations of all\n",
    "centroids inside `centroids` \n",
    "* It should output a one-dimensional array `idx` (which has the same number of elements as `X`) that holds the index  of the closest centroid (a value in $\\{0,...,K-1\\}$, where $K$ is total number of centroids) to every training example . *(Note: The index range 0 to K-1 varies slightly from what is shown in the lectures (i.e. 1 to K) because Python list indices start at 0 instead of 1)*\n",
    "* Specifically, for every example $x^{(i)}$ we set\n",
    "$$c^{(i)} := j \\quad \\mathrm{that \\; minimizes} \\quad ||x^{(i)} - \\mu_j||^2,$$\n",
    "where \n",
    " * $c^{(i)}$ is the index of the centroid that is closest to $x^{(i)}$ (corresponds to `idx[i]` in the starter code), and \n",
    " * $\\mu_j$ is the position (value) of the $j$‚Äôth centroid. (stored in `centroids` in the starter code)\n",
    " * $||x^{(i)} - \\mu_j||$ is the L2-norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def find_closest_centroids(X, centroids):\n",
    "    \"\"\"\n",
    "    Computes the centroid memberships for every example\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray): (m, n) Input values      \n",
    "        centroids (ndarray): (K, n) centroids\n",
    "    \n",
    "    Returns:\n",
    "        idx (array_like): (m,) closest centroids\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Set K\n",
    "    K = centroids.shape[0]\n",
    "\n",
    "    # You need to return the following variables correctly\n",
    "    idx = np.zeros((X.shape[0], ), dtype=int)\n",
    "     ### START CODE HERE ###\n",
    "    \n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "        \n",
    "     ### END CODE HERE ###\n",
    "     return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check your implementation using an example dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Load an example dataset that we will be using\n",
    "X = np.load(\"data/ex7_X.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below prints the first five elements in the variable `X` and the dimensions of the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "print(\"First five elements of X are:\\n\", X[:5]) \n",
    "print('The shape of X is:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Select an initial set of centroids (3 Centroids)\n",
    "initial_centroids = np.array([[3,3], [6,2], [8,5]])\n",
    "\n",
    "# Find closest centroids using initial_centroids\n",
    "idx = find_closest_centroids(X, initial_centroids)\n",
    "\n",
    "# Print closest centroids for the first three elements\n",
    "print(\"First three elements in idx are:\", idx[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>First three elements in idx are<b></td>\n",
    "    <td> [0 2 1] </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Computing Centroid Means\n",
    "\n",
    "Given assignments of every point to a centroid, the second phase of the\n",
    "algorithm recomputes, for each centroid, the mean of the points that\n",
    "were assigned to it.\n",
    "\n",
    "\n",
    "\n",
    "### **Task 2:**\n",
    "\n",
    "Please complete the `compute_centroids` below to recompute the value for each centroid\n",
    "\n",
    "* Specifically, for every centroid $\\mu_k$ we set\n",
    "  \n",
    "\n",
    "$$\n",
    "\\mu_k = \\frac{1}{|C_k|} \\sum_{i \\in C_k} x^{(i)}\n",
    "$$ \n",
    "\n",
    "where \n",
    "\n",
    "* $C_k$ is the set of examples that are assigned to centroid $k$\n",
    "* $|C_k|$ is the number of examples in the set $C_k$\n",
    "\n",
    "\n",
    "* Concretely, if two examples say $x^{(3)}$ and $x^{(5)}$ are assigned to centroid $k=2$,\n",
    "then you should update $\\mu_2 = \\frac{1}{2}(x^{(3)}+x^{(5)})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def compute_centroids(X, idx, K):\n",
    "    \"\"\"\n",
    "    Returns the new centroids by computing the means of the \n",
    "    data points assigned to each centroid.\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):   (m, n) Data points\n",
    "        idx (ndarray): (m,) Array containing index of closest centroid for each \n",
    "                       example in X. Concretely, idx[i] contains the index of \n",
    "                       the centroid closest to example i\n",
    "        K (int):       number of centroids\n",
    "    \n",
    "    Returns:\n",
    "        centroids (ndarray): (K, n) New centroids computed\n",
    "    \"\"\"\n",
    "    \n",
    "    # Useful variables\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    centroids = np.zeros((K, n))\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "  \n",
    "    ### END CODE HERE ## \n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check your implementation by running the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "K = 3\n",
    "centroids = compute_centroids(X, idx, K)\n",
    "\n",
    "print(\"The centroids are:\", centroids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "2.42830111 3.15792418\n",
    "\n",
    "5.81350331 2.63365645\n",
    "\n",
    "7.11938687 3.6166844 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## K-Means on a Sample Dataset\n",
    "\n",
    "After completing the two functions **`find_closest_centroids()`** and **`compute_centroids()`**, the next step is to run the K-Means algorithm on a simple 2D dataset to visualize how it works in practice.  \n",
    "\n",
    "- Review the **`run_kMeans()`** function provided below to understand its workflow.  \n",
    "- Notice that this function repeatedly calls the two functions you implemented inside an iterative loop.  \n",
    "\n",
    "When you execute the code, it will generate a visualization showing how the algorithm progresses at each iteration.  \n",
    "\n",
    "- By the end, your plot should look similar to **Figure 1**.  \n",
    "- The **black X-marks** represent the final centroids located at the center of each colored cluster.  \n",
    "- The **intermediate X-marks**, connected by lines, trace the movement of each centroid across iterations until convergence.  \n",
    "\n",
    "\n",
    "<img src=\"images/figure 1.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "\n",
    "**Note:** You do not need to implement anything for this section ‚Äî simply run the code provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# You do not need to implement anything for this part\n",
    "\n",
    "def run_kMeans(X, initial_centroids, max_iters=100, plot_progress=False):\n",
    "    \"\"\"\n",
    "    Runs the K-Means algorithm on data matrix X, where each row of X\n",
    "    is a single example\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize values\n",
    "    m, n = X.shape\n",
    "    K = initial_centroids.shape[0]\n",
    "    centroids = initial_centroids\n",
    "    previous_centroids = centroids    \n",
    "    idx = np.zeros(m)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Run K-Means\n",
    "    for i in range(max_iters):\n",
    "        \n",
    "        #Output progress every 10 iterations\n",
    "        if i % 10 == 0:\n",
    "            print(\"K-Means iteration %d/%d\" % (i, max_iters-1))\n",
    "        \n",
    "        # For each example in X, assign it to the closest centroid\n",
    "        idx = find_closest_centroids(X, centroids)\n",
    "        \n",
    "        # Optionally plot progress\n",
    "        if plot_progress:\n",
    "            plot_progress_kMeans(X, centroids, previous_centroids, idx, K, i)\n",
    "            previous_centroids = centroids\n",
    "            \n",
    "        # Given the memberships, compute new centroids\n",
    "        centroids = compute_centroids(X, idx, K)\n",
    "    plt.show() \n",
    "    return centroids, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Load an example dataset\n",
    "X = load_data()\n",
    "\n",
    "# Set initial centroids\n",
    "initial_centroids = np.array([[3,3],[6,2],[8,5]])\n",
    "\n",
    "# Number of iterations\n",
    "max_iters = 10\n",
    "\n",
    "# Run K-Means\n",
    "centroids, idx = run_kMeans(X, initial_centroids, plot_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Random initialization\n",
    "\n",
    "The initial assignments of centroids for the example dataset was designed so that you will see the same figure as in Figure 1. In practice, a good strategy for initializing the centroids is to select random examples from the\n",
    "training set.\n",
    "\n",
    "In this part of the exercise, you should understand how the function `kMeans_init_centroids` is implemented.\n",
    "* The code first randomly shuffles the indices of the examples (using `np.random.permutation()`). \n",
    "* Then, it selects the first $K$ examples based on the random permutation of the indices. \n",
    "* This allows the examples to be selected at random without the risk of selecting the same example twice.\n",
    "\n",
    "**Note:** You do not need to implement anything for this section ‚Äî simply run the code provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# You do not need to modify this part\n",
    "\n",
    "def kMeans_init_centroids(X, K):\n",
    "    \"\"\"\n",
    "    This function initializes K centroids that are to be \n",
    "    used in K-Means on the dataset X\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray): Data points \n",
    "        K (int):     number of centroids/clusters\n",
    "    \n",
    "    Returns:\n",
    "        centroids (ndarray): Initialized centroids\n",
    "    \"\"\"\n",
    "    \n",
    "    # Randomly reorder the indices of examples\n",
    "    randidx = np.random.permutation(X.shape[0])\n",
    "    \n",
    "    # Take the first K examples as centroids\n",
    "    centroids = X[randidx[:K]]\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this code, each run of the K-Means algorithm starts with **different randomly initialized centroids**, which can lead to **different clustering results**.  \n",
    "Run the cell below several times and observe how the clusters vary depending on the initial centroid positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Run this cell repeatedly to see different outcomes.\n",
    "\n",
    "# Set number of centroids and max number of iterations\n",
    "K = 3\n",
    "max_iters = 10\n",
    "\n",
    "# Set initial centroids by picking random examples from the dataset\n",
    "initial_centroids = kMeans_init_centroids(X, K)\n",
    "\n",
    "# Run K-Means\n",
    "centroids, idx = run_kMeans(X, initial_centroids, max_iters, plot_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Takeaway**\n",
    "\n",
    "In K-Means, the **initial placement of centroids** plays a crucial role in how the algorithm converges.  \n",
    "If the centroids are poorly initialized, clusters may shift dramatically across iterations, and the algorithm might reach the `max_iter` limit before properly converging ‚Äî resulting in **suboptimal clusters**.  \n",
    "\n",
    "The **[k-means++ algorithm](https://stackoverflow.com/questions/5466323/how-could-one-implement-the-k-means-algorithm)** was designed to address this issue by providing a smarter, more systematic way to initialize centroids.  \n",
    "\n",
    "For additional discussion on why initialization matters, see this [post](https://stats.stackexchange.com/questions/246061/what-are-the-advantages-of-the-pre-defined-initial-centroids-in-clustering).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform K-Means Clustering Using scikit-learn\n",
    "\n",
    "Scikit-learn provides the **[`KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)** class, which offers a convenient and efficient implementation of the K-Means clustering algorithm.  \n",
    "\n",
    "In `sklearn.KMeans`, the **`n_init`** parameter helps improve clustering stability by controlling **how many times the algorithm runs with different random centroid initializations**.  \n",
    "For each run, the algorithm computes the within-cluster sum of squared distances (inertia) and selects the result that yields the **lowest inertia**, i.e., the best clustering outcome.  \n",
    "\n",
    "By default, scikit-learn uses **`init='k-means++'`**, which initializes centroids in a smarter way to speed up convergence and improve accuracy.  \n",
    "The **`n_init`** parameter is typically set to **10**, meaning K-Means runs 10 times with different centroid seeds and returns the best result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 3**\n",
    "\n",
    "Let‚Äôs start by running K-Means with the **default settings** (`n_clusters=3`).  \n",
    "Fill in the missing code sections according to the comments provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run kmeans from sklearn on the X dataset, set k=3\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"KMeans is known to have a memory leak\")\n",
    "\n",
    "\n",
    "# Number of clusters\n",
    "K = 3\n",
    "\n",
    "# Fit KMeans and calculate centroids\n",
    "\n",
    "\n",
    "# get the SSE (sum of squared error), labels, and centroids\n",
    "\n",
    "\n",
    "print(\"The sklearn centroids are:\", sklearn_centroids)\n",
    "print(\"The labels are:\", labels)\n",
    "print(\"The SSE is:\", SSE)\n",
    "\n",
    "\n",
    "#plot the clustering result on the data points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis')\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Selecting the Number of Clusters\n",
    "\n",
    "A key limitation of K-Means is that you must **specify the number of clusters (`k`) in advance** ‚Äî the algorithm cannot automatically determine it from the data.  \n",
    "For example, if you tell K-Means to find **five clusters**, it will proceed to partition the data into five groups, even if that number doesn‚Äôt naturally fit the structure of the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = KMeans(5, random_state=0).fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
    "            s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In practice, methods such as the **Elbow Method** and the **Silhouette Score** are commonly used to help determine a suitable value for **`k`**.\n",
    "\n",
    "\n",
    "#### Elbow Criterion\n",
    "\n",
    "The **Elbow Method** helps determine the optimal number of clusters (**k**) by identifying the point where the **sum of squared errors (SSE)** sharply decreases before leveling off. SSE measures the squared distance between each data point and its assigned cluster centroid.\n",
    "\n",
    "To apply this method, compute SSE for a range of k values and plot the results. As k increases, SSE naturally decreases and approaches zero when each data point forms its own cluster. The optimal number of clusters is typically found at the **‚Äúelbow point‚Äù** of the plot ‚Äî where increasing k yields only marginal improvements in SSE.\n",
    "\n",
    "The `inertia_` attribute returns the **within-cluster sum of squared distances**, which corresponds to the **SSE**_\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Example: given dataset X and number of clusters k\n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Get SSE (Sum of Squared Errors)\n",
    "sse = kmeans.inertia_\n",
    "print(f\"SSE for k={k}: {sse:.2f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 4:** \n",
    "Using the elbow method to find the optimal number of clusters using the following setting\n",
    "\n",
    "```\n",
    "n_clusters in range (1, 10)\n",
    "max_iter=10\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the elbow method to find the optimal number of clusters \n",
    "\n",
    "\n",
    "# plot the SSE for each n_clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhouette Score\n",
    "\n",
    "The [Silhouette Score](https://en.wikipedia.org/wiki/Silhouette_(clustering)) is a metric used to evaluate the quality of clustering results, particularly in K-Means clustering, by measuring the degree of separation between clusters.  \n",
    "It quantifies how similar an object is to its own cluster compared to other clusters.  \n",
    "The score ranges from **-1 to 1**, where **a higher score indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters**, suggesting a good clustering configuration.\n",
    "\n",
    "To use the Silhouette Score to tune the number of clusters (**k**) for K-Means, one typically iterates over different values of **k** and computes the Silhouette Score for each result.  \n",
    "The value of **k** that maximizes the Silhouette Score is usually considered the optimal number of clusters, as it indicates well-separated and cohesive clusters.\n",
    "\n",
    "**Example: Compute the Silhouette Score using scikit-learn**\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Example: given dataset X and number of clusters k\n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "labels = kmeans.fit_predict(X)\n",
    "\n",
    "# Compute Silhouette Score\n",
    "score = silhouette_score(X, labels)\n",
    "print(f\"Silhouette Score for k={k}: {score:.3f}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 5:**\n",
    "Using the Silhouette Score  method to find the optimal number of clusters using the same setting as elbow method\n",
    "\n",
    "```\n",
    "n_clusters in range (2, 8)\n",
    "max_iter=10\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use silhouette score to find the optimal number of clusters\n",
    "# step 1: import library\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# step 2: loop through different number of clusters and get the silhouette score\n",
    "\n",
    "# step 3: plot the silhouette scores for each n_clusters\n",
    "\n",
    "\n",
    "# pick the optimal number of clusters based on the above analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling in K-Means Clustering\n",
    "\n",
    "K-Means clustering relies on **distance calculations** to assign data points to clusters.  \n",
    "If features are on different scales, those with larger numeric ranges can **dominate the distance metric**, leading to biased cluster assignments.  \n",
    "Therefore, **feature scaling** is an essential preprocessing step when applying K-Means.\n",
    "\n",
    "Let's create a synthetic dataset to demonstrate this effect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset with different scales\n",
    "X, _ = make_blobs(n_samples=300, centers=4, random_state=42, cluster_std=[1, 2, 3, 4])\n",
    "# Introduce different scales for features\n",
    "X[:, 0] *= 10  # Increase scale for the first feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-means clustering without feature scaling\n",
    "kmeans_unscaled = KMeans(n_clusters=3, random_state=42)\n",
    "labels_unscaled = kmeans_unscaled.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-means clustering with feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "kmeans_scaled = KMeans(n_clusters=3, random_state=42)\n",
    "labels_scaled = kmeans_scaled.fit_predict(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 4.5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels_unscaled, cmap='viridis')\n",
    "plt.title('K-means Clustering without Feature Scaling')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels_scaled, cmap='viridis')\n",
    "plt.title('K-means Clustering with Feature Scaling')\n",
    "plt.xlabel('Scaled Feature 1')\n",
    "plt.ylabel('Scaled Feature 2')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should observe that **without feature scaling**, K-Means clustering may **misclassify data points** because features with larger scales dominate the distance calculations.  \n",
    "After applying **feature scaling**, the clusters should be **more accurately identified**, since K-Means now evaluates distances on a consistent scale across all features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Demonstration of K-Means Assumptions\n",
    "\n",
    "This section illustrates the conditions under which K-Means performs well versus when it begins to fail. We start with an ideal dataset (compact, well-separated, roughly spherical clusters), then progressively move to datasets that violate its assumptions.\n",
    "\n",
    "###  Ideal (Assumption-Satisfying) Case\n",
    "The following example shows clustered data that is:\n",
    "- Compact and well separated\n",
    "- Approximately circular (isotropic variance)\n",
    "- Similar density and size across clusters\n",
    "\n",
    "K-Means should recover clusters cleanly here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic dataset with clear clusters\n",
    "n_samples = 1000\n",
    "n_components = 4\n",
    "\n",
    "X_blob, true_labels_blob = make_blobs(n_samples=n_samples, centers=n_components, cluster_std=0.50, random_state=0)\n",
    "\n",
    "plt.scatter(X_blob[:,0],X_blob[:,1], c=true_labels_blob, s=50, cmap='viridis')\n",
    "plt.grid()\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit kmeans on the X_blob dataset, k=4\n",
    "kmeans = KMeans(n_clusters=4, random_state=0, n_init=\"auto\").fit(X_blob)\n",
    "\n",
    "cluster_labels_blob = kmeans.predict(X_blob)\n",
    "\n",
    "plt.scatter(X_blob[:,0],X_blob[:,1], c=cluster_labels_blob)\n",
    "plt.grid()\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Why K-Means Works Here\n",
    "K-Means uses Euclidean distance to assign points to the nearest centroid, which implicitly favors circular (spherical) decision regions. When clusters are isotropic and similarly sized/dense, these circular boundaries align well with the data.\n",
    "\n",
    "<img src=\"images/circle_kmeans.png\" width=\"550\" height=\"400\">\n",
    "\n",
    "This simplicity is a strength‚Äîbut it also defines the limits of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Moving Toward Failure Cases\n",
    "Real-world data often violates one or more of these conditions:\n",
    "- Elongated / anisotropic clusters\n",
    "- Nested or non-convex shapes (e.g., concentric circles)\n",
    "- Unequal variance across clusters\n",
    "- Strongly imbalanced cluster sizes\n",
    "\n",
    "Next we explicitly construct datasets to surface these failure modes before introducing alternatives designed to handle them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Demonstration of K-Means Limitations\n",
    "We now build synthetic datasets that break K-Means‚Äô assumptions and visualize how the algorithm behaves. Compare the ground truth with the K-Means assignments to see where and why it fails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's create datasets that violate K-Means assumptions\n",
    "n_samples = 1500\n",
    "random_state = 170\n",
    "transformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]]\n",
    "X, y = make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "\n",
    "X_circ, y_circ = make_circles(n_samples=n_samples, noise=0.1, random_state=0, factor=0.4)\n",
    "X_aniso = np.dot(X, transformation)  # Anisotropic blobs\n",
    "X_varied, y_varied = make_blobs(\n",
    "    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n",
    ")  # Unequal variance\n",
    "X_filtered = np.vstack(\n",
    "    (X[y == 0][:500], X[y == 1][:100], X[y == 2][:10])\n",
    ")  # Unevenly sized blobs\n",
    "y_filtered = [0] * 500 + [1] * 100 + [2] * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 12))\n",
    "\n",
    "axs[0, 0].scatter(X_circ[:, 0], X_circ[:, 1], c=y_circ)\n",
    "axs[0, 0].set_title(\"Nested Circles\")\n",
    "\n",
    "axs[0, 1].scatter(X_aniso[:, 0], X_aniso[:, 1], c=y)\n",
    "axs[0, 1].set_title(\"Anisotropically Distributed Blobs\")\n",
    "\n",
    "axs[1, 0].scatter(X_varied[:, 0], X_varied[:, 1], c=y_varied)\n",
    "axs[1, 0].set_title(\"Unequal Variance\")\n",
    "\n",
    "axs[1, 1].scatter(X_filtered[:, 0], X_filtered[:, 1], c=y_filtered)\n",
    "axs[1, 1].set_title(\"Unevenly Sized Blobs\")\n",
    "\n",
    "plt.suptitle(\"Ground truth clusters\").set_y(0.95)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 6:**\n",
    "Fitting k-means and plot results using the default setting and the correct number of clusters, which can be found in the above data visulization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from the plots above that **K-Means fails in all four cases**:\n",
    "\n",
    "- It struggles with **non-spherical shapes** (e.g., concentric circles).  \n",
    "- It performs poorly on **anisotropic (elongated) distributions**.  \n",
    "- It cannot handle **clusters with unequal variances**.  \n",
    "- It misclassifies **clusters of different sizes**.\n",
    "\n",
    "These limitations arise because K-Means assumes clusters are **spherical, equally sized, and have similar densities**.\n",
    "\n",
    "Next, let's explore how we can **overcome these limitations** using more flexible clustering algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Clustering Unevenly Sized Blobs\n",
    "\n",
    "To handle **unevenly sized blobs**, you can increase the number of random initializations.  \n",
    "In this example, we set **`n_init=15`** to reduce the chance of finding a suboptimal local minimum.\n",
    "\n",
    "\n",
    "### **Task 7**\n",
    "\n",
    "Run K-Means again, this time setting **`n_init=15`**, and plot the resulting clusters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be observed, increasing the number of random initializations in K-means clustering can improve the clustering of unevenly sized blobs by providing the algorithm with more opportunities to explore different starting points for centroids and select the configuration that best fits the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Alternative Clustering Methods to Overcome K-Means Limitations\n",
    "\n",
    "K-means clustering has limitations when dealing with certain types of data distributions, such as nested, anisotropic, and unequal variances. here we propose some other clustering algorithms, such as\n",
    "\n",
    "* Density-Based Spatial Clustering of Applications with Noise (DBSCAN)\n",
    "```\n",
    "from sklearn.cluster import DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "clusters = dbscan.fit_predict(data)\n",
    "```\n",
    "* Agglomerative Hierarchical Clustering\n",
    "```\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=3)\n",
    "clusters = agg_clustering.fit_predict(data)\n",
    "```\n",
    "* Gaussian Mixture Models \n",
    "```\n",
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(n_components=3)\n",
    "gmm.fit(data)\n",
    "clusters = gmm.predict(data)\n",
    "```\n",
    "\n",
    "Let's next apply them to fit the data and see whether they are able to capture the pattern\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 8:**\n",
    "Apply these three algorithms to the challenging datasets and visually compare results with K-Means:\n",
    "- For GaussianMixture and Agglomerative, set the number of clusters equal to K-Means\n",
    "- For DBSCAN, use: `eps=0.11` (Nested Circles), `eps=0.4, min_samples=10` (Anisotropic), `eps=0.9, min_samples=10` (Unequal Variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Models (GMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use dbscan to cluster the X dataset\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative (Hierarchical) Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use agglomerative clustering to cluster the X dataset\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Agglomerative clustering, like K-Means, also struggles with **nested clusters** due to its hierarchical nature.  \n",
    "When using **single linkage**, it tends to merge clusters based on the **minimum distance between their nearest points**, which can lead to incorrect cluster merges in complex or nested structures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Clustering Performance Evaluation\n",
    "\n",
    "In unsupervised learning, we often assess clustering quality along three complementary dimensions:\n",
    "\n",
    "1. **Internal validation** ‚Äì measures structure, cohesion, and separation using only the feature data and predicted cluster labels.  \n",
    "2. **External validation** ‚Äì compares clustering results against known ground truth labels (when available).  \n",
    "3. **Visual / qualitative inspection** ‚Äì uses plots or projections for human interpretation.\n",
    "\n",
    "Below are the most commonly used evaluation methods.\n",
    "\n",
    "\n",
    "\n",
    "###  Internal Metrics (No Ground Truth Needed)\n",
    "\n",
    "**Primary:** **Silhouette Score** *(recommended default)*  \n",
    "- **Intuition:** Measures how similar each point is to its own cluster versus the nearest other cluster.  \n",
    "- **Range:** ‚Äì1 to 1. Higher is better (> 0.5 often good; near 0 ambiguous; negative indicates misassignment).  \n",
    "- **Strengths:** Scale-invariant, easy to compare across different `k` values, and broadly applicable.\n",
    "\n",
    "**Other internal metrics:**\n",
    "- **Davies‚ÄìBouldin Index (DBI):** Average ratio of within-cluster scatter to between-centroid distance. Lower is better. Useful as a secondary check but less interpretable than Silhouette.  \n",
    "- *(Also seen: Calinski‚ÄìHarabasz Index, Dunn Index ‚Äì not used here.)*\n",
    "\n",
    "**Example (Silhouette):**\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "k = 3\n",
    "labels = KMeans(n_clusters=k, random_state=42).fit_predict(X)\n",
    "score = silhouette_score(X, labels)\n",
    "print(f\"Silhouette Score (k={k}): {score:.3f}\")\n",
    "```\n",
    "\n",
    "###  External Metrics (Require Ground Truth Labels)\n",
    "\n",
    "Use these only when **true labels are available** ‚Äî e.g., for benchmarking on synthetic or annotated datasets.\n",
    "\n",
    "**Primary:** **Adjusted Rand Index (ARI)**  \n",
    "- Adjusted for chance; range roughly ‚Äì1 to 1 (1 = perfect agreement, 0 ‚âà random label agreement).  \n",
    "- Robust, symmetric, and widely used.\n",
    "\n",
    "**Other:**  \n",
    "- **Normalized Mutual Information (NMI):** Measures shared information between cluster assignments and true labels, normalized to [0, 1].  \n",
    "  Less sensitive to label permutation than raw mutual information.\n",
    "\n",
    "**Example (ARI & NMI):**\n",
    "```python\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "# y_true: ground truth labels; labels: predicted cluster labels\n",
    "ari = adjusted_rand_score(y_true, labels)\n",
    "nmi = normalized_mutual_info_score(y_true, labels)\n",
    "print(f\"ARI: {ari:.3f}  |  NMI: {nmi:.3f}\")\n",
    "```\n",
    "\n",
    "###  Visual / Qualitative Evaluation\n",
    "\n",
    "- **Scatter plots** using 2D data or PCA / t-SNE / UMAP projections for high-dimensional datasets.  \n",
    "- **Centroid or prototype visualization** (e.g., mean images in image clustering).  \n",
    "- **Cluster size distribution** to detect imbalance or fragmentation.  \n",
    "- **Decision boundary overlays** (when feasible) for visual intuition.\n",
    "\n",
    "\n",
    "\n",
    "### Choosing What to Report\n",
    "\n",
    "- **No ground truth:** Use **Silhouette Score** across candidate **`k`** values (optionally add DBI) and visualizations.  \n",
    "- **With ground truth:** Report **ARI** (and optionally **NMI**) along with an internal metric for structural insight.  \n",
    "- Always include **visualizations** ‚Äî metrics alone can miss geometric or topological issues (e.g., elongated or chained clusters).\n",
    "\n",
    "\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "- Very high Silhouette with very small clusters can indicate **over-segmentation**.  \n",
    "- Low ARI/NMI doesn‚Äôt always mean failure ‚Äî ground truth labels may encode **semantic** rather than geometric differences.  \n",
    "- High internal metrics do not guarantee **usefulness for downstream tasks** ‚Äî always consider domain context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "In this lab, you have\n",
    "* Implemented the one of the most widely used clustering algorithms - K-means.\n",
    "* Learned different perspectives of K-means.\n",
    "* Explored other alternative clutering algorithms to overcome the limitations of K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference:\n",
    "https://www.deeplearning.ai/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
