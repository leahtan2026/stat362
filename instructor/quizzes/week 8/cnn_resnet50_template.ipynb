{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "id": "IyFLQa3FbDYZ"
   },
   "source": [
    "---\n",
    "title: \"Quiz 11: Residual Networks (ResNet-50)\"\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    code-fold: show\n",
    "    self-contained: true\n",
    "    html-math-method: mathml\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-T2yZ3AJSq-"
   },
   "source": [
    "\n",
    "\n",
    "In this lab, you will learn how to build **very deep convolutional neural networks** using **Residual Networks (ResNets)**. While deeper networks can theoretically learn highly complex functions, they are often difficult to train in practice due to challenges such as the **vanishing gradient problem**.  \n",
    "\n",
    "**Residual Networks**, introduced by [He et al. (2015)](https://arxiv.org/pdf/1512.03385.pdf), solved this issue and made it possible to train extremely deep models efficiently.\n",
    "\n",
    "ResNet was the breakthrough architecture that won the **2015 ImageNet Large Scale Visual Recognition Challenge (ILSVRC)** and has influenced almost every deep learning model since then. Residual connections are now used in:\n",
    "\n",
    "- Recurrent models ([Kim et al., 2017](https://arxiv.org/abs/1701.03360), [Prakash et al., 2016](https://arxiv.org/abs/1610.03098))\n",
    "- Transformers ([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762))\n",
    "- Graph neural networks ([Kipf & Welling, 2016](https://arxiv.org/abs/1609.02907))\n",
    "\n",
    "Today, residual ideas form the backbone of modern deep learning architectures.\n",
    "\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "**In this notebook, you will:**\n",
    "\n",
    "- Understand and implement the core building blocks of **ResNets**\n",
    "- Combine these blocks to construct a **50-layer network**\n",
    "- Train this deep network for **image classification**\n",
    "- Load and fine-tune a **pre-trained ResNet-50** model in Keras\n",
    "- Run training on a **GPU** for the first time (deep network â‰ˆ 50 layers!)\n",
    "\n",
    "We will use **Keras with PyTorch as the backend** for this lab.\n",
    "\n",
    "\n",
    "\n",
    "###  Before You Begin\n",
    "\n",
    "Run the cell below to import the required packages.\n",
    "\n",
    "If your laptop has a GPU (e.g., Apple Silicon MacBook or a gaming laptop), the code will automatically use it. Otherwise, it will run on CPU.  \n",
    "\n",
    "> ğŸ’¡ **Tip:** Try running this notebook on both CPU and GPU (e.g., using Google Colab) to experience how significantly a GPU accelerates model training.\n",
    "\n",
    "Let's get started ğŸ‘‡\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1762087013889,
     "user": {
      "displayName": "Lizhen Shi",
      "userId": "11877986201032087944"
     },
     "user_tz": 360
    },
    "id": "saWKpFfNbDYb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND']='torch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8497,
     "status": "ok",
     "timestamp": 1762087022400,
     "user": {
      "displayName": "Lizhen Shi",
      "userId": "11877986201032087944"
     },
     "user_tz": 360
    },
    "id": "hDoHFLZpbDYb"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import torch\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1762087022427,
     "user": {
      "displayName": "Lizhen Shi",
      "userId": "11877986201032087944"
     },
     "user_tz": 360
    },
    "id": "KwEX6EJ-ipqp",
    "outputId": "190af0c9-06f4-4ac6-90da-014a3fa9bfa6"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    device_name = \"Apple Silicon MPS (Metal Performance Shaders)\"\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    device_name = \"CPU\"\n",
    "\n",
    "print(f\"Training on: {device_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1762087022445,
     "user": {
      "displayName": "Lizhen Shi",
      "userId": "11877986201032087944"
     },
     "user_tz": 360
    },
    "id": "ql3WvJGUjGpC",
    "outputId": "a6227a22-8904-488e-bc40-41807ec5b21f"
   },
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "print(f\"Using device: {device}\")\n",
    "# print keras backend\n",
    "\n",
    "print(f\"Keras backend: {keras.backend.backend()}\")\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKBSoaW2bDYb"
   },
   "source": [
    "## 1 - The Problem of Very Deep Neural Networks\n",
    "\n",
    "Modern neural networks have grown dramatically in depth â€” from a few layers (e.g., AlexNet) to hundreds in todayâ€™s architectures. Deeper models can represent more complex functions and learn hierarchical features, from simple edges in early layers to abstract concepts in deeper layers.\n",
    "\n",
    "However, simply stacking more layers does **not** always improve performance. A major challenge is the **vanishing gradient problem**.\n",
    "\n",
    "\n",
    "###  Vanishing Gradients\n",
    "\n",
    "During backpropagation, gradients flow from the final layer back to earlier layers. At each step, gradients are multiplied by weight matrices (and activation derivatives). In very deep networks, this can cause gradients to:\n",
    "\n",
    "-  **Shrink exponentially** â†’ approach zero (**vanishing gradients**)  \n",
    "-  **Grow exponentially** â†’ explode to large values (**exploding gradients**, less common)\n",
    "\n",
    "Even with:\n",
    "\n",
    "- âœ… Proper weight initialization  \n",
    "- âœ… ReLU activations  \n",
    "- âœ… Batch normalization  \n",
    "\n",
    "**Training very deep networks is still harder** because gradients struggle to flow backward through many layers.  \n",
    "Result: **Early layers learn slowly or stop learning** â†’ inefficient training and poor performance.\n",
    "\n",
    "\n",
    "###  What You Observe During Training\n",
    "\n",
    "Gradient norms for earlier layers typically decrease rapidly:\n",
    "\n",
    "> **Earlier layers receive very weak learning signal â†’ slow or stalled learning**\n",
    "\n",
    "*(See figure below â€” earlier layers' gradients decay faster)*\n",
    "\n",
    "<img src=\"images/vanishing_grad_kiank.png\" style=\"width:450px;height:220px;\">\n",
    "<caption><center> <u> <font color='purple'> <b>Figure 1</b> </u><font color='purple'>  : <b>Vanishing gradient</b> <br> The speed of learning decreases very rapidly for the early layers as the network trains </center></caption>\n",
    "\n",
    "**Key takeaway:**  \n",
    "> Gradient flow weakens as networks deepen, making optimization difficult without special architectural mechanisms.\n",
    "\n",
    "\n",
    "\n",
    "###  What's Next?\n",
    "\n",
    "To address this, modern architectures introduce **skip (residual) connections**, which allow gradients to bypass layers and flow more easily.\n",
    "\n",
    "You will now build a **Residual Network (ResNet)** to overcome the vanishing gradient problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fqtc2auEJSrD"
   },
   "source": [
    "## 2 - Building a Residual Network\n",
    "\n",
    "### 2.0 - The Core Idea: Skip Connections\n",
    "\n",
    "In ResNets, a **\"shortcut\"** or **\"skip connection\"** allows information and gradients to flow directly to earlier layers, bypassing intermediate transformations:  \n",
    "\n",
    "<img src=\"images/skip_connection_kiank.png\" style=\"width:650px;height:200px;\">\n",
    "<caption><center> <u> <font color='purple'> <b>Figure 2</b> </u><font color='purple'>  : A ResNet block showing a <b>skip-connection</b> <br> </center></caption>\n",
    "\n",
    "**Left**: Traditional \"main path\" â€” input passes through multiple transformations sequentially.  \n",
    "**Right**: ResNet block â€” the shortcut connection enables the network to learn **residual mappings**.\n",
    "\n",
    "\n",
    "\n",
    "### 2.1 Key Innovation: Learning Residuals Instead of Direct Mappings\n",
    "\n",
    "**Traditional approach**: Learn a direct mapping $H(x)$ from input $x$.\n",
    "\n",
    "**ResNet approach**: Learn a residual function $F(x)$ such that:\n",
    "\n",
    "$$H(x) = F(x) + x$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $x$ is the input (identity/shortcut)\n",
    "- $F(x)$ is the residual learned by the stacked layers\n",
    "- $H(x)$ is the final output\n",
    "\n",
    "**Why is this easier to optimize?**\n",
    "\n",
    "1. **If optimal mapping is close to identity** ($H(x) \\approx x$), it's easier to learn $F(x) \\approx 0$ than to learn $H(x) = x$ directly\n",
    "2. **Gradient flow**: During backpropagation, gradients can flow through both:\n",
    "   - The main path: $\\frac{\\partial F(x)}{\\partial x}$\n",
    "   - The shortcut: $\\frac{\\partial x}{\\partial x} = 1$ (direct path with gradient = 1)\n",
    "\n",
    "This ensures at least some gradient always reaches earlier layers!\n",
    "\n",
    "**Benefits of Residual Learning**\n",
    "\n",
    "| Traditional Deep Networks | Residual Networks (ResNets) |\n",
    "|---------------------------|----------------------------|\n",
    "| Gradients vanish exponentially | Skip connections provide gradient highway |\n",
    "| Harder to optimize as depth increases | Easy to optimize even with 100+ layers |\n",
    "| Adding layers can hurt performance | Adding layers rarely hurts (can learn identity) |\n",
    "| Requires careful initialization | More robust to initialization |\n",
    "\n",
    "### 2.2 Two Types of ResNet Blocks\n",
    "\n",
    "By stacking these ResNet blocks on top of each other, you can form very deep networks. The architecture uses two main block types based on whether input/output dimensions match:\n",
    "\n",
    "1. **Identity Block** â€” When input and output dimensions are the **same**\n",
    "2. **Convolutional Block** â€” When dimensions **differ** (e.g., spatial downsampling or channel expansion)\n",
    "\n",
    "\n",
    "You will now implement both types of ResNet blocks and build a complete ResNet-50 architecture!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "capLZAeBJSrD"
   },
   "source": [
    "#### 2.2.1 - The Identity Block\n",
    "\n",
    "The **identity block** is the standard building block used in ResNets. It's used when the **input and output dimensions are the same** (i.e., $a^{[l]}$ and $a^{[l+2]}$ have identical shapes).\n",
    "\n",
    "##### Basic Structure (2-layer skip)\n",
    "\n",
    "<img src=\"images/idblock2_kiank.png\" style=\"width:650px;height:150px;\">\n",
    "<caption><center> <u> <font color='purple'> <b>Figure 3</b> </u><font color='purple'>  : <b>Identity block.</b> Skip connection \"skips over\" 2 layers. </center></caption>\n",
    "\n",
    "**Key components:**\n",
    "\n",
    "- **Upper path (shortcut)**: Direct connection â€” input passes through unchanged\n",
    "- **Lower path (main path)**: Sequence of transformations (CONV2D â†’ BatchNorm â†’ ReLU)\n",
    "- **Final step**: Add both paths and apply ReLU activation\n",
    "\n",
    "\n",
    "##### Enhanced 3-Layer Identity Block (What You'll Implement)\n",
    "\n",
    "For better feature extraction, we'll implement a more powerful version where the skip connection jumps over **3 layers**:\n",
    "\n",
    "<img src=\"images/idblock3_kiank.png\" style=\"width:650px;height:150px;\">\n",
    "<caption><center> <u> <font color='purple'> <b>Figure 4</b> </u><font color='purple'>  : <b>Identity block.</b> Skip connection \"skips over\" 3 layers.</center></caption>\n",
    "\n",
    "\n",
    "##### Architecture Details\n",
    "\n",
    "The main path consists of three convolutional components with a **bottleneck design**:\n",
    "\n",
    "**First Component** (Dimensionality Reduction):\n",
    "\n",
    "- **CONV2D**: $F_1$ filters, kernel size (1,1), stride (1,1), padding='valid'\n",
    "  - *Purpose*: Reduce channel dimensions (bottleneck)\n",
    "  - *Name*: `conv_name_base + '2a'`\n",
    "- **BatchNorm**: Normalize channels axis â†’ `bn_name_base + '2a'`\n",
    "- **Activation**: ReLU\n",
    "\n",
    "**Second Component** (Feature Extraction):\n",
    "\n",
    "- **CONV2D**: $F_2$ filters, kernel size $(f,f)$, stride (1,1), padding='same'\n",
    "  - *Purpose*: Extract spatial features at reduced dimensionality\n",
    "  - *Name*: `conv_name_base + '2b'`\n",
    "- **BatchNorm**: Normalize channels axis â†’ `bn_name_base + '2b'`\n",
    "- **Activation**: ReLU\n",
    "\n",
    "**Third Component** (Dimensionality Expansion):\n",
    "\n",
    "- **CONV2D**: $F_3$ filters, kernel size (1,1), stride (1,1), padding='valid'\n",
    "  - *Purpose*: Restore original channel dimensions\n",
    "  - *Name*: `conv_name_base + '2c'`\n",
    "- **BatchNorm**: Normalize channels axis â†’ `bn_name_base + '2c'`\n",
    "- **No ReLU** (activation applied after adding shortcut)\n",
    "\n",
    "**Final Step**:\n",
    "\n",
    "- **Add**: Shortcut + Main path output\n",
    "- **Activation**: ReLU on combined result\n",
    "\n",
    "\n",
    "##### Why This Bottleneck Design?\n",
    "\n",
    "The **1Ã—1 â†’ 3Ã—3 â†’ 1Ã—1** pattern is called a **bottleneck architecture**:\n",
    "\n",
    "```\n",
    "Input channels: 256\n",
    "    â†“\n",
    "1Ã—1 conv: 256 â†’ 64   (reduce dimensions - fewer parameters!)\n",
    "    â†“\n",
    "3Ã—3 conv: 64 â†’ 64    (extract features at lower cost)\n",
    "    â†“\n",
    "1Ã—1 conv: 64 â†’ 256   (restore dimensions)\n",
    "    â†“\n",
    "Add shortcut: 256 + 256 â†’ 256\n",
    "```\n",
    "\n",
    "**Benefits**:\n",
    "\n",
    "- âœ… **Fewer parameters**: 3Ã—3 conv operates on fewer channels (64 vs 256)\n",
    "- âœ… **Faster computation**: Reduces computational cost significantly\n",
    "- âœ… **More depth**: Can stack more layers with same parameter budget\n",
    "- âœ… **Better features**: Multiple non-linearities capture complex patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRqAUFa7bDYc"
   },
   "source": [
    "### Task 1: Implement the ResNet Identity Block\n",
    "\n",
    "We've implemented the first component for you. \n",
    "\n",
    "**Your task**: Complete the second component, third component, and final addition step.\n",
    "\n",
    "**Implementation hints:**\n",
    "\n",
    "- Conv2D: [Documentation](https://keras.io/api/layers/convolution_layers/convolution2d/)\n",
    "- BatchNorm: [Documentation](https://faroit.github.io/keras-docs/1.2.2/layers/normalization/)\n",
    "  - Set `axis=3` to normalize the channels axis\n",
    "- Activation: Use `Activation('relu')(X)`\n",
    "- Add layers: [Documentation](https://keras.io/api/layers/merging_layers/add/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1762087022456,
     "user": {
      "displayName": "Lizhen Shi",
      "userId": "11877986201032087944"
     },
     "user_tz": 360
    },
    "id": "RK6Hzgp7bDYc"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, BatchNormalization, Activation, Add\n",
    "\n",
    "# FUNCTION: identity_block\n",
    "\n",
    "def identity_block(X, f, filters, stage, block):\n",
    "    \"\"\"\n",
    "    Implementation of the identity block as defined in Figure 3\n",
    "\n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    stage -- integer, used to name the layers, depending on their position in the network\n",
    "    block -- string/character, used to name the layers, depending on their position in the network\n",
    "\n",
    "    Returns:\n",
    "    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "\n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    # Save the input value. You'll need this later to add back to the main path.\n",
    "    X_shortcut = X\n",
    "\n",
    "    # First component of main path\n",
    "    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a' )(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Second component of main path (â‰ˆ3 lines)\n",
    " \n",
    "\n",
    "    # Third component of main path (â‰ˆ2 lines)\n",
    " \n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (â‰ˆ2 lines)\n",
    " \n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-01T16:22:16.918605Z",
     "iopub.status.busy": "2025-11-01T16:22:16.918403Z",
     "iopub.status.idle": "2025-11-01T16:22:16.946126Z",
     "shell.execute_reply": "2025-11-01T16:22:16.945421Z",
     "shell.execute_reply.started": "2025-11-01T16:22:16.918584Z"
    },
    "executionInfo": {
     "elapsed": 634,
     "status": "ok",
     "timestamp": 1762087023091,
     "user": {
      "displayName": "Lizhen Shi",
      "userId": "11877986201032087944"
     },
     "user_tz": 360
    },
    "id": "xEqWIiYTJSrE",
    "outputId": "b4ed457d-e566-45aa-f498-d389a098a24e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "X = np.random.randn(3, 4, 4, 6)\n",
    "output = identity_block(X, f = 2, filters = [2, 4, 6], stage = 1, block = 'a')\n",
    "\n",
    "print(output.cpu().detach().numpy().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TN_0XuaxJSrE"
   },
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "           A value between <b>100</b> and <b>140</b>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a872056FJSrF"
   },
   "source": [
    "#### 2.2 - The Convolutional Block\n",
    "\n",
    "You've mastered the identity block! Now let's tackle the **convolutional block** â€” the second essential building block in ResNet.\n",
    "\n",
    "##### When to Use Convolutional Blocks\n",
    "\n",
    "Use this block when **input and output dimensions DON'T match**, such as:\n",
    "\n",
    "-  **Spatial downsampling**: Reducing height/width (e.g., 32Ã—32 â†’ 16Ã—16)\n",
    "-  **Channel expansion**: Increasing depth (e.g., 128 channels â†’ 256 channels)\n",
    "-  **Transitioning between stages** in the network\n",
    "\n",
    "\n",
    "##### Key Difference: Shortcut Path Has a Convolution\n",
    "\n",
    "<img src=\"images/convblock_kiank.png\" style=\"width:650px;height:150px;\">\n",
    "<caption><center> <u> <font color='purple'> <b>Figure 5</b> </u><font color='purple'>  : <b>Convolutional block</b> </center></caption>\n",
    "\n",
    "**Unlike the identity block**, the shortcut path contains:\n",
    "\n",
    "- **CONV2D layer**: Transforms input to match output dimensions\n",
    "- **BatchNorm**: Normalizes the transformed shortcut\n",
    "- **No activation**: Shortcut applies only a linear transformation\n",
    "\n",
    "**Why is this needed?**\n",
    "\n",
    "For the final addition to work, both paths must have **matching dimensions**:\n",
    "```python\n",
    "# This MUST work:\n",
    "output = main_path_output + shortcut_output  # Shapes must match!\n",
    "```\n",
    "\n",
    "If input is `(32, 32, 128)` and output should be `(16, 16, 256)`:\n",
    "- Main path reduces spatial dimensions and expands channels\n",
    "- Shortcut path ALSO must transform `(32, 32, 128)` â†’ `(16, 16, 256)`\n",
    "\n",
    "\n",
    "\n",
    "#####  Architecture Details\n",
    "\n",
    "**Main Path** (same as identity block, but with stride):\n",
    "\n",
    "**First Component**:\n",
    "\n",
    "- **CONV2D**: $F_1$ filters, (1,1), **stride (s,s)**, padding='valid'\n",
    "  -  **Stride s**: This is where spatial downsampling happens!\n",
    "  - *Name*: `conv_name_base + '2a'`\n",
    "- **BatchNorm**: `bn_name_base + '2a'`\n",
    "- **Activation**: ReLU\n",
    "\n",
    "**Second Component**:\n",
    "\n",
    "- **CONV2D**: $F_2$ filters, (f,f), stride (1,1), padding='same'\n",
    "  - *Name*: `conv_name_base + '2b'`\n",
    "- **BatchNorm**: `bn_name_base + '2b'`\n",
    "- **Activation**: ReLU\n",
    "\n",
    "**Third Component**:\n",
    "\n",
    "- **CONV2D**: $F_3$ filters, (1,1), stride (1,1), padding='valid'\n",
    "  - *Name*: `conv_name_base + '2c'`\n",
    "- **BatchNorm**: `bn_name_base + '2c'`\n",
    "- **No activation** here\n",
    "\n",
    "\n",
    "\n",
    "**Shortcut Path** (NEW! This is what makes it different):\n",
    "\n",
    "- **CONV2D**: $F_3$ filters, (1,1), **stride (s,s)**, padding='valid'\n",
    "  -  **Same stride as first component** â†’ matches spatial dimensions\n",
    "  -  **Same number of filters as third component** â†’ matches channels\n",
    "  - *Name*: `conv_name_base + '1'`\n",
    "- **BatchNorm**: `bn_name_base + '1'`\n",
    "- **No activation** (keeps it as a linear projection)\n",
    "\n",
    "\n",
    "\n",
    "**Final Step**:\n",
    "\n",
    "- **Add**: Main path + Shortcut path\n",
    "- **Activation**: ReLU on sum\n",
    "\n",
    "\n",
    "\n",
    "#####  Dimension Transformation Example\n",
    "\n",
    "Let's trace dimensions through a convolutional block:\n",
    "\n",
    "**Input**: `(H, W, C_in)` = `(32, 32, 128)`  \n",
    "**Parameters**: `s=2`, `filters=[64, 64, 256]`\n",
    "\n",
    "**Main Path**:\n",
    "```\n",
    "Input:          (32, 32, 128)\n",
    "â†“ Conv 1Ã—1, s=2, 64 filters\n",
    "                (16, 16, 64)   â† stride 2 reduces spatial dims\n",
    "â†“ Conv 3Ã—3, s=1, 64 filters\n",
    "                (16, 16, 64)   â† same padding preserves dims\n",
    "â†“ Conv 1Ã—1, s=1, 256 filters\n",
    "Main output:    (16, 16, 256)  â† expand to final channels\n",
    "```\n",
    "\n",
    "**Shortcut Path**:\n",
    "```\n",
    "Input:          (32, 32, 128)\n",
    "â†“ Conv 1Ã—1, s=2, 256 filters\n",
    "Shortcut:       (16, 16, 256)  â† matches main path!\n",
    "```\n",
    "\n",
    "**Addition**: `(16, 16, 256)` + `(16, 16, 256)` = `(16, 16, 256)` âœ…\n",
    "\n",
    "\n",
    "\n",
    "#####  Why No Activation on Shortcut?\n",
    "\n",
    "The shortcut represents the **identity (or linear projection) of the input**. Adding a non-linearity would:\n",
    "\n",
    "- âŒ Break the gradient highway property\n",
    "- âŒ Make it harder to learn identity mappings\n",
    "- âŒ Reduce the effectiveness of skip connections\n",
    "\n",
    "The shortcut should be a **pure information pathway**, while the main path learns non-linear transformations.\n",
    "\n",
    "\n",
    "\n",
    "### Task 2: Implement the Convolutional Block\n",
    "\n",
    "We've implemented the first component of the main path. \n",
    "\n",
    "**Your task**: Complete the rest.\n",
    "\n",
    "**What to implement**:\n",
    "\n",
    "1. Second and third components of main path\n",
    "2. **Entire shortcut path** (this is the new part!)\n",
    "3. Final addition and activation\n",
    "\n",
    "**Key reminders**:\n",
    "\n",
    "- Use stride `(s,s)` in the FIRST main path conv AND the shortcut conv\n",
    "- Shortcut has $F_3$ filters (same as third component output)\n",
    "- No ReLU on shortcut path or after third component (only after final addition)\n",
    "\n",
    "**References**:\n",
    "\n",
    "- [Conv2D](https://keras.io/layers/convolutional/#conv2d)\n",
    "- [BatchNorm](https://keras.io/layers/normalization/#batchnormalization) (use `axis=3`)\n",
    "- [Activation](https://keras.io/layers/core/#activation): `Activation('relu')(X)`\n",
    "- [Add](https://keras.io/layers/merge/#add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1762087023116,
     "user": {
      "displayName": "Lizhen Shi",
      "userId": "11877986201032087944"
     },
     "user_tz": 360
    },
    "id": "2X_7djiGbDYd"
   },
   "outputs": [],
   "source": [
    "# FUNCTION: convolutional_block\n",
    "\n",
    "def convolutional_block(X, f, filters, stage, block, s = 2):\n",
    "    \"\"\"\n",
    "    Implementation of the convolutional block as defined in Figure 4\n",
    "\n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    stage -- integer, used to name the layers, depending on their position in the network\n",
    "    block -- string/character, used to name the layers, depending on their position in the network\n",
    "    s -- Integer, specifying the stride to be used\n",
    "\n",
    "    Returns:\n",
    "    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "\n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    # Save the input value\n",
    "    X_shortcut = X\n",
    "\n",
    "\n",
    "    ##### MAIN PATH #####\n",
    "    # First component of main path\n",
    "    X = Conv2D(F1, (1, 1), strides = (s,s), name = conv_name_base + '2a' )(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Second component of main path (â‰ˆ3 lines)\n",
    "\n",
    "    # Third component of main path (â‰ˆ2 lines)\n",
    "\n",
    "\n",
    "    ##### SHORTCUT PATH #### (â‰ˆ2 lines)\n",
    "\n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (â‰ˆ2 lines)\n",
    "\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-01T16:22:16.956575Z",
     "iopub.status.busy": "2025-11-01T16:22:16.956147Z",
     "iopub.status.idle": "2025-11-01T16:22:16.986644Z",
     "shell.execute_reply": "2025-11-01T16:22:16.986084Z",
     "shell.execute_reply.started": "2025-11-01T16:22:16.956559Z"
    },
    "executionInfo": {
     "elapsed": 84,
     "status": "ok",
     "timestamp": 1762087023210,
     "user": {
      "displayName": "Lizhen Shi",
      "userId": "11877986201032087944"
     },
     "user_tz": 360
    },
    "id": "4WW_Us7hJSrF",
    "outputId": "da441c16-02f7-49bc-849e-2257fc110b48",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "X = np.random.randn(3, 4, 4, 6)\n",
    "output = convolutional_block(X, f = 2, filters = [2, 4, 6], stage = 1, block = 'a')\n",
    "\n",
    "print(output.detach().cpu().numpy().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yghCJaYUJSrG"
   },
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "           20~30\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgbngQFRbDYd"
   },
   "source": [
    "\n",
    "\n",
    "#### Identity Block vs Convolutional Block â€” Quick Reference\n",
    "\n",
    "| Feature | Identity Block | Convolutional Block |\n",
    "|---------|----------------|---------------------|\n",
    "| **Use Case** | Same input/output dimensions | Different input/output dimensions |\n",
    "| **Shortcut Path** | Direct connection (no layers) | Conv2D + BatchNorm |\n",
    "| **Main Path Stride** | Always (1,1) | First conv uses stride (s,s) |\n",
    "| **When Used** | Within a stage | Between stages (transitions) |\n",
    "| **Purpose** | Deepen network without changing dims | Spatial downsampling or channel expansion |\n",
    "| **Example** | Stage 2, blocks 'b' and 'c' | Stage 2, block 'a' (entry to stage) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mpKEBU8bDYd"
   },
   "source": [
    "### Summary: Building Blocks Mastered!\n",
    "\n",
    "You've now implemented the two fundamental ResNet building blocks:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    IDENTITY BLOCK                           â”‚\n",
    "â”‚  Input (H, W, C) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚\n",
    "â”‚       â”‚                                     â”‚               â”‚\n",
    "â”‚       â”œâ”€â†’ Conv 1Ã—1 (reduce) â”€â†’ BN â”€â†’ ReLU  â”‚               â”‚\n",
    "â”‚       â”œâ”€â†’ Conv 3Ã—3 (extract) â”€â†’ BN â”€â†’ ReLU  â”‚               â”‚\n",
    "â”‚       â””â”€â†’ Conv 1Ã—1 (expand) â”€â†’ BN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤               â”‚\n",
    "â”‚                                             â†“               â”‚\n",
    "â”‚                                         ADD + ReLU          â”‚\n",
    "â”‚                                             â†“               â”‚\n",
    "â”‚                                    Output (H, W, C)         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                 CONVOLUTIONAL BLOCK                         â”‚\n",
    "â”‚  Input (H, W, Câ‚) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚\n",
    "â”‚       â”‚                                     â”‚               â”‚\n",
    "â”‚       â”œâ”€â†’ Conv 1Ã—1 (s=2, reduce) â”€â†’ BN â”€â†’ ReLU             â”‚\n",
    "â”‚       â”œâ”€â†’ Conv 3Ã—3 (s=1, extract) â”€â†’ BN â”€â†’ ReLU             â”‚\n",
    "â”‚       â””â”€â†’ Conv 1Ã—1 (s=1, expand) â”€â†’ BN â”€â”€â”€â”€â”¤               â”‚\n",
    "â”‚                                             â”‚               â”‚\n",
    "â”‚                                             â”‚               â”‚\n",
    "â”‚                          Conv 1Ã—1 (s=2) â”€â†’ BN               â”‚\n",
    "â”‚                                             â†“               â”‚\n",
    "â”‚                                         ADD + ReLU          â”‚\n",
    "â”‚                                             â†“               â”‚\n",
    "â”‚                                  Output (H/2, W/2, Câ‚‚)      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Next step**: Stack these blocks to build a complete ResNet-50!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QB5XuTkbDYe"
   },
   "source": [
    "###  Optional: Visualize Block Dimensions\n",
    "\n",
    "Run the cell below to see how dimensions flow through each block type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-01T16:22:16.987337Z",
     "iopub.status.busy": "2025-11-01T16:22:16.987179Z",
     "iopub.status.idle": "2025-11-01T16:22:17.030153Z",
     "shell.execute_reply": "2025-11-01T16:22:17.029433Z",
     "shell.execute_reply.started": "2025-11-01T16:22:16.987324Z"
    },
    "executionInfo": {
     "elapsed": 178,
     "status": "ok",
     "timestamp": 1762087023389,
     "user": {
      "displayName": "Lizhen Shi",
      "userId": "11877986201032087944"
     },
     "user_tz": 360
    },
    "id": "v0JBPfdXbDYe",
    "outputId": "396c5e8c-9a83-4d45-d1ea-e8a909ceea90",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Optional: Visualize how dimensions transform through ResNet blocks\n",
    "This helps understand the architecture better!\n",
    "\"\"\"\n",
    "def visualize_block_dimensions():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"IDENTITY BLOCK - Dimension Flow\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Test identity block\n",
    "    test_input = np.random.randn(1, 16, 16, 256).astype(\"float32\")\n",
    "    print(f\"Input shape:        {test_input.shape}\")\n",
    "\n",
    "    output = identity_block(test_input, f=3, filters=[64, 64, 256], stage=1, block='test')\n",
    "\n",
    "    if isinstance(output, torch.Tensor):\n",
    "        output_shape = tuple(output.shape)\n",
    "    else:\n",
    "        try:\n",
    "            from keras import ops as Kops\n",
    "            output_shape = tuple(Kops.shape(output))\n",
    "        except:\n",
    "            output_shape = output.shape\n",
    "\n",
    "    print(f\"Output shape:       {output_shape}\")\n",
    "    print(f\"âœ“ Dimensions preserved (identity block characteristic)\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"CONVOLUTIONAL BLOCK - Dimension Flow\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Test convolutional block\n",
    "    test_input2 = np.random.randn(1, 32, 32, 128).astype(\"float32\")\n",
    "    print(f\"Input shape:        {test_input2.shape}\")\n",
    "\n",
    "    output2 = convolutional_block(test_input2, f=3, filters=[64, 64, 256], stage=2, block='test', s=2)\n",
    "\n",
    "    if isinstance(output2, torch.Tensor):\n",
    "        output2_shape = tuple(output2.shape)\n",
    "    else:\n",
    "        try:\n",
    "            from keras import ops as Kops\n",
    "            output2_shape = tuple(Kops.shape(output2))\n",
    "        except:\n",
    "            output2_shape = output2.shape\n",
    "\n",
    "    print(f\"Output shape:       {output2_shape}\")\n",
    "    print(f\"âœ“ Spatial dims reduced by stride 2: {test_input2.shape[1]} â†’ {output2_shape[1]}\")\n",
    "    print(f\"âœ“ Channels expanded: {test_input2.shape[3]} â†’ {output2_shape[3]}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "#  run visualization\n",
    "visualize_block_dimensions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xsT4iTSvJSrG"
   },
   "source": [
    "## 3 - Building your first ResNet model (50 layers)\n",
    "\n",
    "You now have the necessary blocks to build a very deep ResNet. The following figure describes in detail the architecture of this neural network. \"ID BLOCK\" in the diagram stands for \"Identity block,\" and \"ID BLOCK x3\" means you should stack 3 identity blocks together.\n",
    "\n",
    "<img src=\"images/resnet_kiank.png\" style=\"width:850px;height:150px;\">\n",
    "<caption><center> <u> <font color='purple'> <b>Figure 5 </b></u><font color='purple'>  : <b>ResNet-50 model</b> </center></caption>\n",
    "\n",
    "The details of this ResNet-50 model are:\n",
    "\n",
    "- Zero-padding pads the input with a pad of (3,3)\n",
    "- Stage 1:\n",
    "    - The 2D Convolution has 64 filters of shape (7,7) and uses a stride of (2,2). Its name is \"conv1\".\n",
    "    - BatchNorm is applied to the channels axis of the input.\n",
    "    - MaxPooling uses a (3,3) window and a (2,2) stride.\n",
    "- Stage 2:\n",
    "    - The convolutional block uses three set of filters of size [64,64,256], \"f\" is 3, \"s\" is 1 and the block is \"a\".\n",
    "    - The 2 identity blocks use three set of filters of size [64,64,256], \"f\" is 3 and the blocks are \"b\" and \"c\".\n",
    "- Stage 3:\n",
    "    - The convolutional block uses three set of filters of size [128,128,512], \"f\" is 3, \"s\" is 2 and the block is \"a\".\n",
    "    - The 3 identity blocks use three set of filters of size [128,128,512], \"f\" is 3 and the blocks are \"b\", \"c\" and \"d\".\n",
    "- Stage 4:\n",
    "    - The convolutional block uses three set of filters of size [256, 256, 1024], \"f\" is 3, \"s\" is 2 and the block is \"a\".\n",
    "    - The 5 identity blocks use three set of filters of size [256, 256, 1024], \"f\" is 3 and the blocks are \"b\", \"c\", \"d\", \"e\" and \"f\".\n",
    "- Stage 5:\n",
    "    - The convolutional block uses three set of filters of size [512, 512, 2048], \"f\" is 3, \"s\" is 2 and the block is \"a\".\n",
    "    - The 2 identity blocks use three set of filters of size [512, 512, 2048], \"f\" is 3 and the blocks are \"b\" and \"c\".\n",
    "- The 2D Average Pooling uses a window of shape (2,2) and its name is \"avg_pool\".\n",
    "- The flatten doesn't have any hyperparameters or name.\n",
    "- The Fully connected layer reduces its input to the number of classes using a softmax activation. Its name should be `'fc' + str(classes)`.\n",
    "\n",
    "### Task 3: Implement the ResNet with 50 layers described in the figure above.\n",
    "We have implemented Stages 1 and 2. Please implement the rest. (The syntax for implementing Stages 3-5 should be quite similar to that of Stage 2.) Make sure you follow the naming convention in the text above.\n",
    "\n",
    "You'll need to use this function:\n",
    "\n",
    "- Average pooling [see reference](https://keras.io/layers/pooling/#averagepooling2d)\n",
    "\n",
    "Here're some other functions we used in the code below:\n",
    "\n",
    "- Conv2D: [See reference](https://keras.io/layers/convolutional/#conv2d)\n",
    "- BatchNorm: [See reference](https://keras.io/layers/normalization/#batchnormalization) (axis: Integer, the axis that should be normalized (typically the features axis))\n",
    "- Zero padding: [See reference](https://keras.io/layers/convolutional/#zeropadding2d)\n",
    "- Max pooling: [See reference](https://keras.io/layers/pooling/#maxpooling2d)\n",
    "- Fully connected layer: [See reference](https://keras.io/layers/core/#dense)\n",
    "- Addition: [See reference](https://keras.io/layers/merge/#add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T16:22:17.031180Z",
     "iopub.status.busy": "2025-11-01T16:22:17.030907Z",
     "iopub.status.idle": "2025-11-01T16:22:17.041193Z",
     "shell.execute_reply": "2025-11-01T16:22:17.040469Z",
     "shell.execute_reply.started": "2025-11-01T16:22:17.031161Z"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1762087023428,
     "user": {
      "displayName": "Lizhen Shi",
      "userId": "11877986201032087944"
     },
     "user_tz": 360
    },
    "id": "B0DYVVd5JSrG",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# FUNCTION: ResNet50\n",
    "\n",
    "from keras.layers import Input, ZeroPadding2D, AveragePooling2D, Flatten, Dense, Conv2D, MaxPooling2D\n",
    "from keras.models import Model\n",
    "\n",
    "def ResNet50(input_shape = (64, 64, 3), classes = 6):\n",
    "    \"\"\"\n",
    "    Implementation of the popular ResNet50 the following architecture:\n",
    "    CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n",
    "    -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> TOPLAYER\n",
    "\n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "    classes -- integer, number of classes\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "\n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "\n",
    "    # Stage 1\n",
    "    X = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1')(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "\n",
    "    # Stage 2\n",
    "    X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 2, block='a', s = 1)\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Stage 3 (â‰ˆ4 lines)\n",
    "\n",
    "\n",
    "    # Stage 4 (â‰ˆ6 lines)\n",
    "\n",
    "\n",
    "    # Stage 5 (â‰ˆ3 lines)\n",
    "\n",
    "\n",
    "    # AVGPOOL (â‰ˆ1 line). Use \"X = AveragePooling2D(...)(X)\"\n",
    "\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation='softmax', name='fc' + str(classes))(X)\n",
    "\n",
    "\n",
    "    # Create model\n",
    "    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pPSVrhLJSrH"
   },
   "source": [
    "Run the following code to build the model's graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T16:22:17.042035Z",
     "iopub.status.busy": "2025-11-01T16:22:17.041817Z",
     "iopub.status.idle": "2025-11-01T16:22:17.248250Z",
     "shell.execute_reply": "2025-11-01T16:22:17.247693Z",
     "shell.execute_reply.started": "2025-11-01T16:22:17.042004Z"
    },
    "executionInfo": {
     "elapsed": 548,
     "status": "ok",
     "timestamp": 1762087023987,
     "user": {
      "displayName": "Lizhen Shi",
      "userId": "11877986201032087944"
     },
     "user_tz": 360
    },
    "id": "eQQsD_8OJSrH",
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = ResNet50(input_shape = (64, 64, 3), classes = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2025-11-01T16:22:17.249363Z",
     "iopub.status.busy": "2025-11-01T16:22:17.249110Z",
     "iopub.status.idle": "2025-11-01T16:22:17.386862Z",
     "shell.execute_reply": "2025-11-01T16:22:17.386166Z",
     "shell.execute_reply.started": "2025-11-01T16:22:17.249340Z"
    },
    "executionInfo": {
     "elapsed": 593,
     "status": "ok",
     "timestamp": 1762087024583,
     "user": {
      "displayName": "Lizhen Shi",
      "userId": "11877986201032087944"
     },
     "user_tz": 360
    },
    "id": "8xOT1JLJm62L",
    "outputId": "cebb8953-39c5-4eec-d3b7-06e242392257",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOfK5CE4JSrH"
   },
   "source": [
    "### Compile the Model \n",
    "\n",
    "Run the following code to compile your model\n",
    "\n",
    "   - `accuracy` as the evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-01T16:22:17.388147Z",
     "iopub.status.busy": "2025-11-01T16:22:17.387910Z",
     "iopub.status.idle": "2025-11-01T16:22:17.396038Z",
     "shell.execute_reply": "2025-11-01T16:22:17.395250Z",
     "shell.execute_reply.started": "2025-11-01T16:22:17.388117Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1762087024589,
     "user": {
      "displayName": "Lizhen Shi",
      "userId": "11877986201032087944"
     },
     "user_tz": 360
    },
    "id": "wAErExUp1DbH",
    "outputId": "1a8dfa5b-8289-4187-e709-7e552ce6f4e3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nCompiling model...\")\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"\\nFinished Compiling model...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4v1Z40IBJSrH"
   },
   "source": [
    "The model is now ready to be trained. The only thing you need is a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wrknOBTm62M"
   },
   "source": [
    "## 4 - Applying the ResNet model (50 layers) to the sign dataset\n",
    "\n",
    "Let's load the SIGNS Dataset.\n",
    "\n",
    "<img src=\"images/signs_data_kiank.png\" style=\"width:450px;height:250px;\">\n",
    "<caption><center> <u> <font color='purple'> <b>Figure 6</b> </u><font color='purple'>  : <b>SIGNS dataset</b> </center></caption>\n",
    "\n",
    "> **Note:**  \n",
    "> In this dataset, the labels are **one-hot encoded**, so we compile the model with:\n",
    ">\n",
    "> ```python\n",
    "> loss='categorical_crossentropy'\n",
    "> ```\n",
    ">\n",
    "> If your labels are instead **integer class indices** (e.g., `0, 1, 2, ...`), then you should use:\n",
    ">\n",
    "> ```python\n",
    "> loss='sparse_categorical_crossentropy'\n",
    "> ```\n",
    ">\n",
    "> Both losses compute the same objective â€” they simply expect labels in different formats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T16:22:17.416326Z",
     "iopub.status.busy": "2025-11-01T16:22:17.416033Z",
     "iopub.status.idle": "2025-11-01T16:22:17.421678Z",
     "shell.execute_reply": "2025-11-01T16:22:17.420919Z",
     "shell.execute_reply.started": "2025-11-01T16:22:17.416310Z"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1762087024591,
     "user": {
      "displayName": "Lizhen Shi",
      "userId": "11877986201032087944"
     },
     "user_tz": 360
    },
    "id": "NVD775qvJz82",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import math\n",
    "\n",
    "def load_dataset():\n",
    "    # Loading the training and test datasets, you may need to change the path to where you have saved these files\n",
    "    train_dataset = h5py.File('train_signs.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('test_signs.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "\n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "\n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)].T\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-01T16:22:17.424521Z",
     "iopub.status.busy": "2025-11-01T16:22:17.424255Z",
     "iopub.status.idle": "2025-11-01T16:22:17.614615Z",
     "shell.execute_reply": "2025-11-01T16:22:17.613928Z",
     "shell.execute_reply.started": "2025-11-01T16:22:17.424505Z"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1762087024614,
     "user": {
      "displayName": "Lizhen Shi",
      "userId": "11877986201032087944"
     },
     "user_tz": 360
    },
    "id": "CQmHW0zbJSrI",
    "outputId": "722c478c-cf55-465b-e8e0-0e084d9b84f5",
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()\n",
    "print(X_train_orig.shape, Y_train_orig.shape, X_test_orig.shape, Y_test_orig.shape, classes.shape)\n",
    "# shuffle the training dataset\n",
    "np.random.seed(1)\n",
    "m = X_train_orig.shape[0]\n",
    "permutation = list(np.random.permutation(m))\n",
    "X_train_orig = X_train_orig[permutation]\n",
    "Y_train_orig = Y_train_orig[:, permutation]\n",
    "\n",
    "# Normalize image vectors\n",
    "X_train = X_train_orig/255.\n",
    "X_test = X_test_orig/255.\n",
    "\n",
    "# Convert training and test labels to one hot matrices\n",
    "Y_train = convert_to_one_hot(Y_train_orig, 6).T\n",
    "Y_test = convert_to_one_hot(Y_test_orig, 6).T\n",
    "\n",
    "val_size = int(len(X_train)*0.8)\n",
    "X_val = X_train[val_size:]\n",
    "Y_val = Y_train[val_size:]\n",
    "X_train = X_train[:val_size]\n",
    "Y_train = Y_train[:val_size]\n",
    "\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_val shape: \" + str(X_val.shape))\n",
    "print (\"Y_val shape: \" + str(Y_val.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovzv-uF4bDYf"
   },
   "source": [
    "### Train Your Model for 2 Epochs\n",
    "\n",
    "To begin, train the model for just **2 epochs**.  \n",
    "This short run helps ensure that everything is working correctly and that your device (CPU or GPU) can train the model without any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-01T16:22:17.615514Z",
     "iopub.status.busy": "2025-11-01T16:22:17.615323Z",
     "iopub.status.idle": "2025-11-01T16:22:26.712904Z",
     "shell.execute_reply": "2025-11-01T16:22:26.712096Z",
     "shell.execute_reply.started": "2025-11-01T16:22:17.615499Z"
    },
    "executionInfo": {
     "elapsed": 15818,
     "status": "ok",
     "timestamp": 1762087040433,
     "user": {
      "displayName": "Lizhen Shi",
      "userId": "11877986201032087944"
     },
     "user_tz": 360
    },
    "id": "Gc6exfqxJSrI",
    "outputId": "2dab7b7d-6812-4170-c1c5-161694d8b3d5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train for 2 epochs\n",
    "model.fit(X_train, Y_train, epochs = 2, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Bl3yYqgJSrI"
   },
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <b> Epoch 1/2 </b>\n",
    "        </td>\n",
    "        <td>\n",
    "           loss: between 1 and 5, acc: between 0.2 and 0.5, although your results can be different from ours.\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <b> Epoch 2/2 </b>\n",
    "        </td>\n",
    "        <td>\n",
    "           loss: between 1 and 5, acc: between 0.3 and 0.7, you should see your loss decreasing and the accuracy increasing.\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fT45gc7YJSrI"
   },
   "source": [
    "### Task 5: Output the test accuracy of this model (trained on only two epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-01T16:22:26.713937Z",
     "iopub.status.busy": "2025-11-01T16:22:26.713704Z",
     "iopub.status.idle": "2025-11-01T16:22:26.942782Z",
     "shell.execute_reply": "2025-11-01T16:22:26.942074Z",
     "shell.execute_reply.started": "2025-11-01T16:22:26.713921Z"
    },
    "executionInfo": {
     "elapsed": 343,
     "status": "ok",
     "timestamp": 1762087040778,
     "user": {
      "displayName": "Lizhen Shi",
      "userId": "11877986201032087944"
     },
     "user_tz": 360
    },
    "id": "H2vAN2KIJSrI",
    "outputId": "4f813a9c-662d-4ebf-cdd7-bdc6d5db9041",
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "preds = model.evaluate(X_test, Y_test)\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-sPlyaNsJSrJ"
   },
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <b>Test Accuracy</b>\n",
    "        </td>\n",
    "        <td>\n",
    "           between 0.16 and 0.25\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MaK7CoQKJSrJ"
   },
   "source": [
    "For two epochs, You can see that it achieves poor performances. Let's next train it for 50 epochs and see whether the performance will get a lot better.\n",
    "\n",
    "To prevent overfitting, we use the `EarlyStopping` callback. Training will stop if the validation loss does not improve for several epochs, and the best weights will be restored.\n",
    "\n",
    "### Train the model for 50 epochs and output the test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1762087040789,
     "user": {
      "displayName": "Lizhen Shi",
      "userId": "11877986201032087944"
     },
     "user_tz": 360
    },
    "id": "E7kGv8PDbDYf"
   },
   "outputs": [],
   "source": [
    "# Adding a Callback for Early Stopping to prevent overfitting\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, start_from_epoch=10,\n",
    "                           restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-01T16:22:26.943869Z",
     "iopub.status.busy": "2025-11-01T16:22:26.943603Z",
     "iopub.status.idle": "2025-11-01T16:23:36.449487Z",
     "shell.execute_reply": "2025-11-01T16:23:36.448706Z",
     "shell.execute_reply.started": "2025-11-01T16:22:26.943846Z"
    },
    "executionInfo": {
     "elapsed": 189114,
     "status": "ok",
     "timestamp": 1762087662136,
     "user": {
      "displayName": "Lizhen Shi",
      "userId": "11877986201032087944"
     },
     "user_tz": 360
    },
    "id": "J3t5-NxNJSrJ",
    "outputId": "175d8113-9580-4a03-a1a2-b04501e04689",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train a fresh model for 18 epochs with validation split\n",
    "import time\n",
    "start_time = time.time()\n",
    "history_model = model.fit(X_train, Y_train, epochs = 50, batch_size = 32, validation_data=(X_val, Y_val),callbacks =[early_stop])\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed = end_time - start_time\n",
    "mins = int(elapsed // 60)\n",
    "secs = elapsed % 60\n",
    "\n",
    "print(f\"Total training time: {mins} min {secs:.1f} sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Draw the training curve and find the best epoch (lowest validation loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 244,
     "status": "ok",
     "timestamp": 1762087664624,
     "user": {
      "displayName": "Lizhen Shi",
      "userId": "11877986201032087944"
     },
     "user_tz": 360
    },
    "id": "xuXDjyZ6laq2",
    "outputId": "c348e74f-2c74-4aeb-84a0-d7addc56a86c"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: output the test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-01T16:23:36.801301Z",
     "iopub.status.busy": "2025-11-01T16:23:36.801067Z",
     "iopub.status.idle": "2025-11-01T16:23:37.028506Z",
     "shell.execute_reply": "2025-11-01T16:23:37.027847Z",
     "shell.execute_reply.started": "2025-11-01T16:23:36.801280Z"
    },
    "executionInfo": {
     "elapsed": 300,
     "status": "ok",
     "timestamp": 1762087693628,
     "user": {
      "displayName": "Lizhen Shi",
      "userId": "11877986201032087944"
     },
     "user_tz": 360
    },
    "id": "BtTYouvgJSrJ",
    "outputId": "c5d6a426-f25f-4e90-a2b0-018f8f11fedc",
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUMIXchRJSrJ"
   },
   "source": [
    "## 5 - `ResNet50` in Keras\n",
    "ResNet50 is a powerful convolutional neural network architecture commonly used for image classification tasks. Keras offers the [ResNet50 model](https://keras.io/api/applications/) pre-trained on the ImageNet dataset, facilitating rapid development and achieving high accuracy on various image recognition tasks.\n",
    "\n",
    "In the next step, we will leverage this pre-trained ResNet50 model to compare its performance with our custom implementation. This comparative analysis will provide insights into the effectiveness and efficiency of both approaches for image classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T16:23:37.029371Z",
     "iopub.status.busy": "2025-11-01T16:23:37.029157Z",
     "iopub.status.idle": "2025-11-01T16:23:37.238173Z",
     "shell.execute_reply": "2025-11-01T16:23:37.237620Z",
     "shell.execute_reply.started": "2025-11-01T16:23:37.029357Z"
    },
    "executionInfo": {
     "elapsed": 237,
     "status": "ok",
     "timestamp": 1762087218468,
     "user": {
      "displayName": "Lizhen Shi",
      "userId": "11877986201032087944"
     },
     "user_tz": 360
    },
    "id": "PqhEPScnm62O",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "\n",
    "# Load the ResNet50 model without the top classification layer\n",
    "base_model = ResNet50(weights=None, include_top=False, input_shape=(64, 64, 3))\n",
    "\n",
    "X = base_model.output\n",
    "X = AveragePooling2D(pool_size=(2, 2), name = 'avg_pool')(X)\n",
    "X = Flatten()(X)\n",
    "\n",
    "# Add a softmax layer for the number of classes (6 in this case)\n",
    "predictions = Dense(6, activation='softmax')(X)\n",
    "\n",
    "# Create the final model\n",
    "ResNet50_keras_model = Model(inputs=base_model.input, outputs=predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbJbUVpOm62O"
   },
   "source": [
    "### Task 6: Using the same model configuration you implemented above, train the model and report the test accuracy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-01T16:25:49.154304Z",
     "iopub.status.busy": "2025-11-01T16:25:49.153715Z",
     "iopub.status.idle": "2025-11-01T16:26:59.308706Z",
     "shell.execute_reply": "2025-11-01T16:26:59.308071Z",
     "shell.execute_reply.started": "2025-11-01T16:25:49.154277Z"
    },
    "executionInfo": {
     "elapsed": 200610,
     "status": "ok",
     "timestamp": 1762087981385,
     "user": {
      "displayName": "Lizhen Shi",
      "userId": "11877986201032087944"
     },
     "user_tz": 360
    },
    "id": "ZExTvZsem62O",
    "outputId": "e651ce80-27b2-4729-916e-2f3223bf3319",
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 271,
     "status": "ok",
     "timestamp": 1762087984502,
     "user": {
      "displayName": "Lizhen Shi",
      "userId": "11877986201032087944"
     },
     "user_tz": 360
    },
    "id": "GM2ZOzhQnQne",
    "outputId": "f0f3411c-55c0-4ca4-adf3-d94a2cfe6074"
   },
   "outputs": [],
   "source": [
    "# Draw the training curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T16:24:57.017596Z",
     "iopub.status.busy": "2025-11-01T16:24:57.017086Z",
     "iopub.status.idle": "2025-11-01T16:24:57.153356Z",
     "shell.execute_reply": "2025-11-01T16:24:57.152633Z",
     "shell.execute_reply.started": "2025-11-01T16:24:57.017576Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1762087378490,
     "user": {
      "displayName": "Lizhen Shi",
      "userId": "11877986201032087944"
     },
     "user_tz": 360
    },
    "id": "cn2a6n-Bm62O",
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-01T16:27:11.218472Z",
     "iopub.status.busy": "2025-11-01T16:27:11.218183Z",
     "iopub.status.idle": "2025-11-01T16:27:11.448585Z",
     "shell.execute_reply": "2025-11-01T16:27:11.447727Z",
     "shell.execute_reply.started": "2025-11-01T16:27:11.218451Z"
    },
    "executionInfo": {
     "elapsed": 405,
     "status": "ok",
     "timestamp": 1762087995998,
     "user": {
      "displayName": "Lizhen Shi",
      "userId": "11877986201032087944"
     },
     "user_tz": 360
    },
    "id": "ve1MyUrxm62c",
    "outputId": "4df1bdc6-df02-4779-eb51-f14ad8a836f2",
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VuYtSFUFJSrL"
   },
   "source": [
    "\n",
    "**What you should remember:**\n",
    "\n",
    "- Very deep \"plain\" networks don't work in practice because they are hard to train due to vanishing gradients.  \n",
    "- The skip-connections help to address the Vanishing Gradient problem. They also make it easy for a ResNet block to learn an identity function.\n",
    "- There are two main type of blocks: The identity block and the convolutional block.\n",
    "- Very deep Residual Networks are built by stacking these blocks together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJ2L1JkLm62c"
   },
   "source": [
    "##  Congratulations\n",
    "\n",
    "ResNet50 is a powerful model for image classification when it is trained for an adequate number of iterations. We hope you can use what you've learnt and apply it to your own classification problem to perform state-of-the-art accuracy.\n",
    "\n",
    "Congratulations on finishing this lab! You've now implemented a state-of-the-art image classification system!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_npenJSyJSrL"
   },
   "source": [
    "## References\n",
    "\n",
    "This notebook introduces the ResNet architecture originally proposed by **He et al. (2015)**.  \n",
    "The implementation draws inspiration from official resources and community examples, including work by **FranÃ§ois Chollet** and the **DeepLearning.AI** team.\n",
    "\n",
    "**Primary Source**\n",
    "- Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.  \n",
    "  *Deep Residual Learning for Image Recognition*, 2015.  \n",
    "  <https://arxiv.org/abs/1512.03385>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zspnAJn6m62d"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1AHisSMD4kuG_Hq23DoNiK1x8xD9xRY25",
     "timestamp": 1761956341482
    }
   ]
  },
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "OEpi5",
   "launcher_item_id": "jK9EQ"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8626891,
     "sourceId": 13579157,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
