{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1e48d114",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"HW2_3: Financial Sentiment Analysis Using Deep Learning\"\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    code-fold: show\n",
    "    self-contained: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dfa191",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates various approaches to financial sentiment analysis using the Financial PhraseBank dataset. We explore multiple machine learning and deep learning techniques, progressing from basic to advanced methods:\n",
    "\n",
    "1. Traditional ML with embeddings\n",
    "   - Multi-layer Perceptron (MLP)\n",
    "   - XGBoost (Gradient Boosting)\n",
    "\n",
    "2. Sequential Models\n",
    "   - Gated Recurrent Unit\n",
    "   - Bidirectional Gated RNNs\n",
    "   - Transformer Architecture\n",
    "\n",
    "3. Modern Approaches\n",
    "   - Pre-trained FinBERT\n",
    "\n",
    "Each section includes implementation, evaluation metrics, and performance comparison. By the end of this notebook, you'll understand the trade-offs between different approaches to sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a077f2e",
   "metadata": {},
   "source": [
    "## Setup and Requirements\n",
    "\n",
    "This notebook requires the following packages:\n",
    "\n",
    "- transformers (4.30+)\n",
    "- datasets (2.10+)\n",
    "- nltk (3.8+)\n",
    "- xgboost (2.0+)\n",
    "- sentence_transformers (2.2+)\n",
    "- keras (3.0+)\n",
    "- torch (2.0+)\n",
    "\n",
    "We'll first set up our environment and configure the backend (TensorFlow or PyTorch) based on the operating system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d62866e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, BertTokenizerFast, BertModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import xgboost as xgb\n",
    "\n",
    "import datasets\n",
    "import keras\n",
    "import torch\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='tensorflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49dedea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import os\n",
    "\n",
    "# Check operating system\n",
    "is_macos = platform.system() == 'Darwin'\n",
    "\n",
    "if is_macos:\n",
    "    # For MacOS, use TensorFlow backend\n",
    "    os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "    import tensorflow as tf\n",
    "    print(\"Using TensorFlow backend on MacOS\")\n",
    "    device = '/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'\n",
    "else:\n",
    "    # For other OS, use PyTorch backend\n",
    "    os.environ['KERAS_BACKEND'] = 'torch'\n",
    "    import torch\n",
    "    print(\"Using PyTorch backend\")\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5031b17",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "The Financial PhraseBank dataset is a comprehensive collection of financial texts with sentiment annotations:\n",
    "\n",
    "- **Size**: 4,840 sentences from English financial news articles\n",
    "- **Labels**: Three-class sentiment classification (positive, negative, neutral)\n",
    "- **Annotation**: 5-8 annotators per sentence with varying agreement levels\n",
    "- **Source**: News articles about OMX Helsinki-listed companies\n",
    "- **Annotators**: Finance experts (researchers and master's students from Aalto University)\n",
    "\n",
    "### Importance\n",
    "The dataset addresses the scarcity of high-quality annotated data in financial sentiment analysis, providing a benchmark for model evaluation. The annotations are particularly valuable as they come from individuals with domain expertise in finance.\n",
    "\n",
    "### Dataset Variants\n",
    "The dataset offers different versions based on annotator agreement:\n",
    "- 50% agreement (used in this notebook)\n",
    "- 66% agreement\n",
    "- 75% agreement\n",
    "- 100% agreement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda72e5d",
   "metadata": {},
   "source": [
    "### Data Loading, Inspection, and Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b87e96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset\n",
    "def load_financial_phrasebank():\n",
    "    \"\"\"\n",
    "    Load and preprocess the Financial PhraseBank dataset from Hugging Face.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed dataset with text and sentiment columns\n",
    "    \"\"\"\n",
    "    # Load dataset from Hugging Face\n",
    "    dataset = datasets.load_dataset('financial_phrasebank', 'sentences_50agree', trust_remote_code=True)\n",
    "    \n",
    "    # Convert to pandas DataFrame\n",
    "    data = pd.DataFrame(dataset['train'])\n",
    "    \n",
    "    # Map numerical labels to text labels\n",
    "    data['sentiment'] = data['label'].map({0: 'negative', 1: 'neutral', 2: 'positive'})\n",
    "    \n",
    "    # Rename sentence column to text for consistency\n",
    "    data = data.rename(columns={'sentence': 'text'})\n",
    "    \n",
    "    # Display basic information about the dataset\n",
    "    print(\"Dataset Shape:\", data.shape)\n",
    "    print(\"\\nSample entries:\")\n",
    "    display(data.head())\n",
    "    \n",
    "    # Show class distribution\n",
    "    print(\"\\nSentiment Distribution:\")\n",
    "    sentiment_dist = data['sentiment'].value_counts(normalize=True)\n",
    "    print(sentiment_dist.round(3))\n",
    "    \n",
    "    # Create a pie chart of sentiment distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.pie(sentiment_dist.values, labels=sentiment_dist.index, autopct='%1.1f%%')\n",
    "    plt.title('Sentiment Distribution in Financial PhraseBank')\n",
    "    plt.show()\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading Financial PhraseBank dataset from Hugging Face...\")\n",
    "data = load_financial_phrasebank()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cb84bd",
   "metadata": {},
   "source": [
    "### Data Splitting\n",
    "\n",
    "The dataset doesn't come with a predefined train/validation/test split, so we'll create our own using an 80/10/10 split ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbcbcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/validation/test splits\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def prepare_data_splits(data, test_size=0.2, val_size=0.25, random_state=42):\n",
    "    \"\"\"\n",
    "    Prepare data splits and encode labels.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): Input dataset\n",
    "        test_size (float): Proportion of data for testing\n",
    "        val_size (float): Proportion of training data for validation\n",
    "        random_state (int): Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "    \"\"\"\n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(data['sentiment'])\n",
    "    \n",
    "    # First split: training + validation vs test\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        data['text'].values,\n",
    "        y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    # Second split: training vs validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp,\n",
    "        y_temp,\n",
    "        test_size=val_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(\"Data split sizes:\")\n",
    "    print(f\"Training: {len(X_train)} samples\")\n",
    "    print(f\"Validation: {len(X_val)} samples\")\n",
    "    print(f\"Test: {len(X_test)} samples\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Create data splits\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = prepare_data_splits(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1c812c",
   "metadata": {},
   "source": [
    "## Data Modeling\n",
    "\n",
    "To address the sentiment analysis task using machine learning, we employ modern approaches that leverage **text embeddings** â€” a cornerstone technique in deep learningâ€“based natural language processing.\n",
    "\n",
    "In this notebook, we explore three complementary modeling directions:\n",
    "\n",
    "1. **Sentence-level embeddings**  \n",
    "   Each sentence is encoded into a fixed-size vector (e.g., 384 dimensions).  \n",
    "   These embeddings are used as input features for traditional machine learning classifiers such as **MLP** and **XGBoost**.\n",
    "\n",
    "2. **Token-level embeddings with sequence models**  \n",
    "   Texts are tokenized into individual tokens, each represented as an embedding vector forming a sequence.  \n",
    "   Sequence models such as **RNN-based architectures** and **Transformers** are then employed to capture temporal and contextual dependencies within the text.\n",
    "\n",
    "3. **Pretrained sentiment models (inference only)**  \n",
    "   We also apply **pretrained sentiment analysis models**, such as **FinBERT**, directly to the test data without additional training.  \n",
    "   This approach allows us to benchmark performance against state-of-the-art, domain-specific models trained on large financial text corpora.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a2004a",
   "metadata": {},
   "source": [
    "### Sentence and Word Embeddings Using SentenceTransformers\n",
    "\n",
    "[**SentenceTransformers**](https://www.sbert.net/) is a powerful library that provides state-of-the-art pretrained models for generating text embeddings.  \n",
    "In this section, we use it for two complementary purposes:\n",
    "\n",
    "1. **Sentence-level embeddings**  \n",
    "   - Convert each full sentence into a fixed-length vector (384 dimensions).  \n",
    "   - Capture the semantic meaning of complete financial statements.  \n",
    "   - Serve as input features for traditional machine learning models such as **XGBoost** and **MLP**.\n",
    "\n",
    "2. **Word-level embeddings**  \n",
    "   - Generate embeddings for individual tokens within each sentence.  \n",
    "   - Preserve sequential and contextual information for deep learning models.  \n",
    "   - Used as input for **GRU**, **LSTM**, and **Transformer** architectures.\n",
    "\n",
    "#### Model Details\n",
    "- **Base model:** `all-MiniLM-L6-v2`  \n",
    "- **Embedding dimension:** 384  \n",
    "- **Advantages:**  \n",
    "  - Fast inference time  \n",
    "  - Strong semantic performance on financial text  \n",
    "  - Memory efficient and lightweight\n",
    "\n",
    "These embeddings form the **foundation for all classification models** developed in this notebook.  \n",
    "Next, we'll load the SentenceTransformer model and ensure it runs on the appropriate compute device (CPU or GPU).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1500836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SentenceTransformer model with appropriate device placement\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Handle device placement based on backend\n",
    "    if is_macos:\n",
    "        with tf.device(device):\n",
    "            model = embedding_model\n",
    "    else:\n",
    "        model = embedding_model.to(device)\n",
    "        \n",
    "    print(\"Embedding model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading embedding model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab22c4d",
   "metadata": {},
   "source": [
    "### Traditional Models with Sentence-Level Embeddings\n",
    "\n",
    "In this section, we implement and evaluate **traditional machine learning models** using the generated sentence embeddings.\n",
    "\n",
    "#### Generating Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61221db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sentence embeddings for train, validation, and test sets\n",
    "print(\"Generating sentence embeddings for all splits...\")\n",
    "\n",
    "# Generate embeddings for each split\n",
    "X_train_emb = embedding_model.encode(\n",
    "    X_train,\n",
    "    batch_size=32,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "X_val_emb = embedding_model.encode(\n",
    "    X_val,\n",
    "    batch_size=32,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "X_test_emb = embedding_model.encode(\n",
    "    X_test,\n",
    "    batch_size=32,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "print(\"\\nEmbedding shapes:\")\n",
    "print(f\"Training: {X_train_emb.shape}\")\n",
    "print(f\"Validation: {X_val_emb.shape}\")\n",
    "print(f\"Test: {X_test_emb.shape}\")\n",
    "print(f\"\\nEach sentence is represented by a {X_train_emb.shape[1]}-dimensional vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c183c02f",
   "metadata": {},
   "source": [
    "Let's visualize the sentence embeddings using **t-SNE**, a dimensionality reduction technique for visualization that you'll learn in the **Unsupervised Learning** module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1e79cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embeddings using t-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Combine embeddings and labels for visualization\n",
    "all_embeddings = np.vstack([X_train_emb, X_val_emb, X_test_emb])\n",
    "all_labels = np.concatenate([y_train, y_val, y_test])\n",
    "\n",
    "# Reduce dimensionality to 2D using t-SNE\n",
    "print(\"Reducing dimensionality with t-SNE...\")\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embeddings_2d = tsne.fit_transform(all_embeddings)\n",
    "\n",
    "# Create scatter plot with discrete colors\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Define colors and labels for each sentiment\n",
    "colors = ['red', 'gray', 'green']\n",
    "sentiments = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "for i, (color, sentiment) in enumerate(zip(colors, sentiments)):\n",
    "    mask = all_labels == i\n",
    "    plt.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1],\n",
    "               c=color, label=sentiment, alpha=0.6)\n",
    "\n",
    "plt.title('t-SNE Visualization of Sentence Embeddings')\n",
    "plt.xlabel('t-SNE dimension 1')\n",
    "plt.ylabel('t-SNE dimension 2')\n",
    "plt.legend(title='Sentiment')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df86f020",
   "metadata": {},
   "source": [
    "### Task 1: MLP for sentiment classification\n",
    "\n",
    "Construct a feed-forward neural network that achieves a test accuracy of at least 74.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfe8a51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27c5925c",
   "metadata": {},
   "source": [
    "### Task 2: Gradient Boosted Trees for Multiclass Classification\n",
    "\n",
    "Explore optimized gradient boosting algorithms â€” **XGBoost**, **LightGBM**, or **CatBoost** â€” to build a model that achieves performance comparable to the **MLP** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23ed27f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d75657d9",
   "metadata": {},
   "source": [
    "### Sequential Models with Word-Level Embeddings\n",
    "\n",
    "Unlike traditional machine learning approaches that use fixed sentence embeddings, sequential models process text as a **sequence of words**, preserving word order and contextual relationships.  \n",
    "In this section, we implement three powerful sequential architectures:\n",
    "\n",
    "1. **Gated RNNs**  \n",
    "   - A variant of the vanilla RNN  \n",
    "   - Efficient for processing sequential data  \n",
    "   - Effective at capturing short-term dependencies  \n",
    "\n",
    "2. **Bidirectional Gated RNNs**  \n",
    "   - Process sequences in both forward and backward directions  \n",
    "   - Capture both past and future context  \n",
    "   - Well-suited for natural language understanding tasks  \n",
    "\n",
    "3. **Transformer** *(implemented for you)*  \n",
    "   - Employs a self-attention mechanism  \n",
    "   - Enables parallel processing of sequences  \n",
    "   - Achieves state-of-the-art performance on modern NLP tasks\n",
    "`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83e0462",
   "metadata": {},
   "source": [
    "#### Text Preprocessing for Sequential Models\n",
    "\n",
    "To prepare text data for sequential models, we need to:\n",
    "\n",
    "1. **Tokenization**\n",
    "   - Break sentences into individual words/tokens\n",
    "   - Preserve meaningful linguistic units\n",
    "   - Handle special characters and punctuation\n",
    "\n",
    "2. **Sequence Padding**\n",
    "   - Ensure all sequences have same length\n",
    "   - Pad shorter sequences with zeros\n",
    "   - Truncate longer sequences to max length\n",
    "\n",
    "3. **Word-Level Embeddings**\n",
    "   - Convert each token to fixed-size vector\n",
    "   - Capture semantic relationships between words\n",
    "   - Use pre-trained embeddings for better representation\n",
    "\n",
    "We'll use NLTK's tokenizer for word segmentation and then create fixed-length sequences suitable for our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ff1d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')  # Download the necessary data for tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize('hello world!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49531bcc",
   "metadata": {},
   "source": [
    "Check the sentence length distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e2e3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sentence length distribution\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate sentence lengths using the tokenized text\n",
    "sentence_lengths = [len(word_tokenize(text)) for text in X_train]\n",
    "\n",
    "# Create figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Histogram with KDE\n",
    "sns.histplot(sentence_lengths, bins=30, kde=True, color='skyblue', ax=ax1)\n",
    "ax1.set_title('Distribution of Sentence Lengths')\n",
    "ax1.set_xlabel('Number of Words')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot 2: Box plot\n",
    "sns.boxplot(y=sentence_lengths, color='skyblue', ax=ax2)\n",
    "ax2.set_title('Sentence Length Box Plot')\n",
    "ax2.set_ylabel('Number of Words')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistical summary\n",
    "print(\"\\nSentence Length Statistics:\")\n",
    "print(f\"Minimum length: {min(sentence_lengths)} words\")\n",
    "print(f\"Maximum length: {max(sentence_lengths)} words\")\n",
    "print(f\"Mean length: {sum(sentence_lengths)/len(sentence_lengths):.1f} words\")\n",
    "print(f\"Median length: {sorted(sentence_lengths)[len(sentence_lengths)//2]} words\")\n",
    "print(f\"\\nPercentiles:\")\n",
    "for p in [10, 25, 75, 90, 95]:\n",
    "    percentile = np.percentile(sentence_lengths, p)\n",
    "    print(f\"{p}th percentile: {percentile:.1f} words\")\n",
    "\n",
    "# Calculate how many sentences would be affected by different max lengths\n",
    "for max_len in [30, 40, 50, 60]:\n",
    "    affected = sum(1 for x in sentence_lengths if x > max_len)\n",
    "    print(f\"\\nSentences longer than {max_len} words: {affected} ({affected/len(sentence_lengths):.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7072b4",
   "metadata": {},
   "source": [
    "sentence different length, we need to pad them to make them the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464a5889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and padding function for sequences\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "def process_text_sequences(texts, max_length=50, padding='post', truncating='post'):\n",
    "    \"\"\"\n",
    "    Convert texts to padded sequences of tokens.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings\n",
    "        max_length: Maximum sequence length (default: 50)\n",
    "        padding: 'pre' or 'post' padding (default: 'post')\n",
    "        truncating: 'pre' or 'post' truncation (default: 'post')\n",
    "    \n",
    "    Returns:\n",
    "        List of padded token sequences\n",
    "    \"\"\"\n",
    "    # Tokenize all texts\n",
    "    sequences = [word_tokenize(text) for text in texts]\n",
    "    \n",
    "    # Convert tokens to numpy arrays with padding\n",
    "    padded_sequences = pad_sequences(\n",
    "        sequences=[s[:max_length] for s in sequences],  # Truncate if needed\n",
    "        maxlen=max_length,\n",
    "        padding=padding,\n",
    "        truncating=truncating,\n",
    "        dtype=object,  # Use object dtype for string tokens\n",
    "        value=''  # Use empty string as padding token\n",
    "    )\n",
    "    \n",
    "    return padded_sequences\n",
    "\n",
    "# Process training, validation, and test sets\n",
    "print(\"Processing sequences...\")\n",
    "X_train_padded = process_text_sequences(X_train)\n",
    "X_val_padded = process_text_sequences(X_val)\n",
    "X_test_padded = process_text_sequences(X_test)\n",
    "\n",
    "# Print shapes and example\n",
    "print(f\"\\nSequence shapes:\")\n",
    "print(f\"Training: {X_train_padded.shape}\")\n",
    "print(f\"Validation: {X_val_padded.shape}\")\n",
    "print(f\"Test: {X_test_padded.shape}\")\n",
    "\n",
    "# Show an example sequence\n",
    "print(\"\\nExample padded sequence:\")\n",
    "print(X_train_padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3c6c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import cache\n",
    "@cache\n",
    "def encode_words(text):\n",
    "    return embedding_model.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ede69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Convert padded sequences to embeddings\n",
    "def create_embedding_sequences(sequences, max_length=50):\n",
    "    \"\"\"\n",
    "    Convert sequences of words to sequences of embeddings using cached word vectors.\n",
    "    Ensures all sequences have the same length through padding.\n",
    "    \n",
    "    Args:\n",
    "        sequences: List of word sequences (padded)\n",
    "        max_length: Maximum sequence length (default: 50)\n",
    "    Returns:\n",
    "        numpy array of shape (n_sequences, max_length, embedding_dim)\n",
    "    \"\"\"\n",
    "    n_sequences = len(sequences)\n",
    "    embedding_dim = 384  # Known dimension from the model\n",
    "    \n",
    "    # Initialize the output array with zeros\n",
    "    embedded_seqs = np.zeros((n_sequences, max_length, embedding_dim))\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        # Get embeddings for non-empty tokens\n",
    "        valid_tokens = [word for word in seq if word != '']\n",
    "        # Truncate if necessary\n",
    "        valid_tokens = valid_tokens[:max_length]\n",
    "        # Create embeddings for valid tokens\n",
    "        seq_embeddings = [encode_words(word) for word in valid_tokens]\n",
    "        \n",
    "        # Add embeddings to the output array with padding\n",
    "        for j, embedding in enumerate(seq_embeddings):\n",
    "            if j < max_length:\n",
    "                embedded_seqs[i, j] = embedding\n",
    "    \n",
    "    return embedded_seqs\n",
    "\n",
    "# Create embeddings for each split\n",
    "print(\"Creating word embeddings for training set...\")\n",
    "X_train_embedded = create_embedding_sequences(X_train_padded)\n",
    "print(\"Creating word embeddings for validation set...\")\n",
    "X_val_embedded = create_embedding_sequences(X_val_padded)\n",
    "print(\"Creating word embeddings for test set...\")\n",
    "X_test_embedded = create_embedding_sequences(X_test_padded)\n",
    "\n",
    "print(\"\\nEmbedding shapes:\")\n",
    "print(f\"Training: {X_train_embedded.shape}\")\n",
    "print(f\"Validation: {X_val_embedded.shape}\")\n",
    "print(f\"Test: {X_test_embedded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393353dc",
   "metadata": {},
   "source": [
    "### Task 3: Build a Gated RNN Model (LSTM or GRU) to Achieve Test Accuracy Above 75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546c3b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15d65076",
   "metadata": {},
   "source": [
    "### Task 4: Build a Bidirectional Gated RNN Model to Achieve Test Accuracy Above 76%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1c4d52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de268220",
   "metadata": {},
   "source": [
    "At this point, youâ€™ve completed all your assigned tasks.  \n",
    "To give you a taste of **state-of-the-art models and large language models (LLMs)**, the **Transformer** and **pretrained models** have been implemented for you â€” you only need to run the provided code.  \n",
    "\n",
    "ðŸŽ‰ **Congratulations on reaching this stage!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4a3b9c",
   "metadata": {},
   "source": [
    "\n",
    "### Transformer Model for Sentiment Analysis\n",
    "\n",
    "The Transformer architecture, introduced in the paper *â€œAttention Is All You Needâ€* (2017), represents a major breakthrough in deep learning for natural language processing tasks.  \n",
    "Unlike RNN-based models (LSTM or GRU), Transformers process entire sequences in parallel using self-attention mechanisms.\n",
    "\n",
    "For this particular problem, however, a Transformer is somewhat **overkill**, as the dataset is relatively small and the task can be solved effectively with simpler models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de35d43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required layers for transformer\n",
    "from keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Input, GlobalAveragePooling1D, Add, Dense\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Multi-head self attention with residual connection\n",
    "    attention_output = MultiHeadAttention(\n",
    "        num_heads=num_heads, \n",
    "        key_dim=head_size, \n",
    "        dropout=dropout\n",
    "    )(inputs, inputs)\n",
    "    attention_output = Dropout(dropout)(attention_output)\n",
    "    attention_output = Add()([inputs, attention_output])  # Residual connection\n",
    "    x1 = LayerNormalization(epsilon=1e-6)(attention_output)\n",
    "\n",
    "    # Feed-forward network with residual connection\n",
    "    ffn_output = Dense(ff_dim, activation=\"relu\")(x1)\n",
    "    ffn_output = Dense(inputs.shape[-1])(ffn_output)\n",
    "    ffn_output = Dropout(dropout)(ffn_output)\n",
    "    x2 = Add()([x1, ffn_output])  # Residual connection\n",
    "    return LayerNormalization(epsilon=1e-6)(x2)\n",
    "\n",
    "# Build the transformer model\n",
    "def build_transformer_model(\n",
    "    max_seq_length=50,\n",
    "    embed_dim=384,\n",
    "    num_heads=4,  # Reduced from 8 to better match embedding dimension\n",
    "    ff_dim=512,   # Increased feed-forward dimension\n",
    "    num_transformer_blocks=2,\n",
    "    mlp_units=[256, 128],  # Increased MLP units\n",
    "    dropout=0.1,\n",
    "    mlp_dropout=0.1,\n",
    "):\n",
    "    inputs = Input(shape=(max_seq_length, embed_dim))\n",
    "    x = inputs\n",
    "\n",
    "    # Add positional encoding if needed (not required if using pre-trained embeddings)\n",
    "    # x = positional_encoding(x)\n",
    "\n",
    "    # Transformer blocks\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, embed_dim // num_heads, num_heads, ff_dim, dropout)\n",
    "\n",
    "    # Global average pooling\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # MLP layers with residual connections\n",
    "    for dim in mlp_units:\n",
    "        x_residual = x\n",
    "        x = Dense(dim, activation=\"relu\")(x)\n",
    "        x = Dropout(mlp_dropout)(x)\n",
    "        if x_residual.shape[-1] == dim:  # Only add if dimensions match\n",
    "            x = Add()([x, x_residual])\n",
    "        x = LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "    # Classification layer\n",
    "    outputs = Dense(3, activation=\"softmax\")(x)\n",
    "\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# Create and compile the model\n",
    "print(\"Building transformer model...\")\n",
    "transformer_model = build_transformer_model(\n",
    "    max_seq_length=50,\n",
    "    embed_dim=384,\n",
    "    num_heads=4,      # Adjusted to work better with embedding dimension\n",
    "    ff_dim=512,       # Increased feed-forward dimension\n",
    "    num_transformer_blocks=2,\n",
    "    mlp_units=[256, 128],  # Increased MLP units\n",
    "    dropout=0.1,\n",
    "    mlp_dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a740b792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "transformer_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Model summary\n",
    "transformer_model.summary()\n",
    "\n",
    "# Add early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model with early stopping\n",
    "print(\"\\nTraining Transformer model...\")\n",
    "history = transformer_model.fit(\n",
    "    X_train_embedded,  # Use embedded sequences\n",
    "    y_train,\n",
    "    batch_size=32,\n",
    "    epochs=50,\n",
    "    validation_data=(X_val_embedded, y_val),  # Use embedded validation sequences\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109992ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "plt.title('Transformer Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.title('Transformer Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_loss, test_accuracy = transformer_model.evaluate(X_test_embedded, y_test, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675f548e",
   "metadata": {},
   "source": [
    "### Pre-trained FinBERT for Financial Sentiment Analysis\n",
    "\n",
    "FinBERT is a domain-specific BERT model pre-trained on financial text, making it particularly well-suited for financial sentiment analysis. \n",
    "\n",
    "#### Model Architecture and Pre-training\n",
    "- **Base Architecture**: Built on BERT-base (12 layers, 768 hidden units, 12 attention heads)\n",
    "- **Pre-training Data**: \n",
    "  - Financial news articles and corporate reports\n",
    "  - SEC 10-K/Q filings\n",
    "  - Earnings call transcripts\n",
    "  - ~46GB of financial text\n",
    "- **Pre-training Tasks**:\n",
    "  1. Masked Language Modeling (MLM)\n",
    "  2. Next Sentence Prediction (NSP)\n",
    "\n",
    "#### Domain Adaptation\n",
    "FinBERT adapts BERT for financial text by:\n",
    "- Learning financial jargon and terminology\n",
    "- Understanding context-specific meanings\n",
    "- Capturing market-specific sentiment nuances\n",
    "- Recognizing financial entities and relationships\n",
    "\n",
    "#### Implementation Notes\n",
    "In our implementation:\n",
    "- Using ProsusAI's FinBERT version\n",
    "- Fine-tuned for 3-class sentiment classification\n",
    "- Labels mapped as: [positive â†’ 2, negative â†’ 0, neutral â†’ 1]\n",
    "- Batch processing for efficiency\n",
    "\n",
    "This pre-trained model should provide better performance than our custom models, especially given our relatively small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93df8649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FinBERT model for sequence classification\n",
    "print(\"Loading FinBERT model...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"ProsusAI/finbert\", \n",
    "    num_labels=3,\n",
    "    cache_dir=\"models\"  # Cache the model locally\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb281a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FinBERT tokenizer\n",
    "print(\"Loading FinBERT tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"ProsusAI/finbert\",\n",
    "    cache_dir=\"models\"  # Cache the tokenizer locally\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ad736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for batch prediction\n",
    "def predict_sentiment_batch(texts, batch_size=32):\n",
    "    \"\"\"\n",
    "    Predict sentiment for a batch of texts using FinBERT.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of texts to analyze\n",
    "        batch_size (int): Size of batches for processing\n",
    "    \n",
    "    Returns:\n",
    "        list: Predicted sentiment labels (0: negative, 1: neutral, 2: positive)\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        \n",
    "        # Tokenize batch\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            batch_predictions = outputs.logits.argmax(dim=1).tolist()\n",
    "            \n",
    "            # Convert FinBERT labels to our format\n",
    "            # FinBERT: [positive, negative, neutral] -> Our format: [negative, neutral, positive]\n",
    "            batch_predictions = [2 if p == 0 else 0 if p == 1 else 1 for p in batch_predictions]\n",
    "            predictions.extend(batch_predictions)\n",
    "            \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db6b4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy array to list of strings and make predictions\n",
    "print(\"Making predictions on test set...\")\n",
    "X_test_list = X_test.tolist()  # Convert numpy array to list\n",
    "predicted = predict_sentiment_batch(X_test_list)\n",
    "\n",
    "# Display sample predictions\n",
    "print(\"\\nSample predictions:\")\n",
    "for i in range(5):\n",
    "    print(f\"\\nText: {X_test_list[i]}\")\n",
    "    sentiment_map = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
    "    print(f\"Predicted: {sentiment_map[predicted[i]]}\")\n",
    "    print(f\"Actual: {sentiment_map[y_test[i]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a80bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive evaluation of FinBERT model\n",
    "print(\"FinBERT Model Evaluation\")\n",
    "print(\"-----------------------\")\n",
    "\n",
    "# Calculate overall metrics\n",
    "accuracy = (np.array(predicted) == y_test).mean()\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Create and plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_test, predicted)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Negative', 'Neutral', 'Positive'],\n",
    "            yticklabels=['Negative', 'Neutral', 'Positive'])\n",
    "plt.title('FinBERT Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade30046",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate and display misclassified examples\n",
    "print(\"\\nExample Misclassifications:\")\n",
    "sentiment_map = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
    "misclassified = np.where(np.array(predicted) != y_test)[0]\n",
    "for idx in misclassified[:5]:  # Show first 5 misclassifications\n",
    "    print(f\"\\nText: {X_test[idx]}\")\n",
    "    print(f\"Predicted: {sentiment_map[predicted[idx]]}\")\n",
    "    print(f\"Actual: {sentiment_map[y_test[idx]]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat362-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
