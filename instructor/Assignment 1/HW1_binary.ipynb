{
 "cells": [
  {
   "cell_type": "raw",
   "id": "2c15dac0-bea3-4ef7-8968-798d69efefd3",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"HW1: L2 Regularized Logistic Regression\"\n",
    "subtitle: \"From Scratch Implementation to Real-World Heart Disease Prediction\"\n",
    "number-sections: true\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    code-fold: show\n",
    "    self-contained: true\n",
    "    html-math-method: mathml \n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5de41f-a061-4451-8b50-41aec0f95c7b",
   "metadata": {},
   "source": [
    "## Instructions {-}\n",
    "\n",
    "1. You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity. \n",
    "\n",
    "2. Write your code in the **Code cells** and your answers in the **Markdown cells** of the Jupyter notebook. Ensure that the solution is written neatly enough to for the graders to understand and follow.\n",
    "\n",
    "3. Use [Quarto](https://quarto.org/docs/output-formats/html-basics.html) to render the **.ipynb** file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: `quarto render filename.ipynb --to html`. Submit the HTML file.\n",
    "\n",
    "4. The assignment is worth 100 points, and is due on **18th October 2025 at 11:59 pm**. \n",
    "\n",
    "5. **Five points are properly formatting the assignment**. The breakdown is as follows:\n",
    "    - Must be an HTML file rendered using Quarto **(1 point)**. *If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file.* \n",
    "    - No name can be written on the assignment, nor can there be any indicator of the studentâ€™s identityâ€”e.g. printouts of the working directory should not be included in the final submission.  **(1 point)**\n",
    "    - There arenâ€™t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there arenâ€™t long printouts of which iteration a loop is on, there arenâ€™t long sections of commented-out code, etc.) **(1 point)**\n",
    "    - Final answers to each question are written in the Markdown cells. **(1 point)**\n",
    "    - There is no piece of unnecessary / redundant code, and no unnecessary / redundant text. **(1 point)**\n",
    "\n",
    "6.  The maximum possible score in the assigment is 100  + 15 (bonus task) + 5 (AI usage disclosure)= 120 out of 100."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37860adb",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "- **Reinforce Knowledge of Gradient Descent:** Apply your understanding of gradient descent to classification problems by implementing logistic regression and softmax regression from scratch.\n",
    "- **Hands-on Implementation:** Build classification models manually to gain deeper insights into their mathematical foundations and working principles.\n",
    "- **Explore Customization Options:** Learn how implementing models from scratch allows you to:\n",
    "  - Adjust and optimize model parameters for specific requirements.\n",
    "  - Add features or constraints that might not be possible with standard libraries.\n",
    "- **Compare with Pre-built Models:** Use scikit-learnâ€™s logistic regression as a baseline to evaluate the performance and efficiency of your custom implementation. This will help you understand when to use custom models and when to leverage pre-built ones.\n",
    "- **Prepare for Real-world Scenarios:** Understand the scenarios where off-the-shelf models are not sufficient, allowing you to confidently tackle complex machine learning problems and create novel solutions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f39540-d78d-4cec-8ee5-51229dcad5c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.2' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"/Users/ltan/Documents/STAT 362-0/stat362-1/.venv/bin/python\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1f9358-3bba-42c7-9b0c-8792a1327918",
   "metadata": {},
   "source": [
    "## Refreshing the Foundations: Sigmoid & Logistic Regression\n",
    "\n",
    "### Why Logistic Regression? The Classification Challenge\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/image-96.webp\" alt=\"Logistic Cost Function\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "**The Problem**: Linear regression gives us $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot  \\mathbf{x}^{(i)} + b$, but for classification:\n",
    "\n",
    "- **Linear regression outputs**: Any real number (-âˆž to +âˆž)\n",
    "- **Classification needs**: Probabilities between 0 and 1\n",
    "- **Binary targets**: $y \\in \\{0, 1\\}$ (e.g., Disease/No Disease, Spam/Not Spam)\n",
    "\n",
    "**The Solution**: Transform linear outputs into probabilities using the **sigmoid function**, creating a model that outputs meaningful probabilities for binary classification decisions.\n",
    "\n",
    "**Real-World Context**: In this assignment, you'll predict heart disease risk where:\n",
    "- **Input**: Patient medical data (age, cholesterol, chest pain type, etc.)\n",
    "- **Output**: Probability of heart disease (0 = healthy, 1 = disease)\n",
    "- **Goal**: Build a model doctors can trust for medical decision-making\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b347e1-8907-490d-bf66-15f8a8c9b568",
   "metadata": {},
   "source": [
    "### The Sigmoid Function: Mathematical Bridge to Probabilities\n",
    "\n",
    "**Mathematical Definition:**\n",
    "$$g(z) = \\frac{1}{1 + e^{-z}} \\tag{1}$$\n",
    "\n",
    "**What This Function Does:**\n",
    "- **Input $z$**: Any real number (-âˆž to +âˆž) from linear model\n",
    "- **Output $g(z)$**: Always between 0 and 1 (perfect for probabilities!)\n",
    "- **S-shaped curve**: Smooth transition from 0 to 1\n",
    "\n",
    "**Key Properties:**\n",
    "- **$g(0) = 0.5$**: Neutral prediction (equal probability of both classes)\n",
    "- **$g(z) \\to 1$** as $z \\to +\\infty$: Strong positive evidence â†’ high probability  \n",
    "- **$g(z) \\to 0$** as $z \\to -\\infty$: Strong negative evidence â†’ low probability\n",
    "- **Always smooth**: No sudden jumps, making it perfect for gradient descent optimization\n",
    "\n",
    "**Practical Interpretation:**\n",
    "In logistic regression, $z = \\mathbf{w} \\cdot \\mathbf{x} + b$ represents the \"log-odds\" or \"logit\":\n",
    "- **Positive $z$**: Model leans toward class 1 (e.g., \"has disease\")\n",
    "- **Negative $z$**: Model leans toward class 0 (e.g., \"healthy\")  \n",
    "- **Magnitude of $|z|$**: Confidence level of the prediction\n",
    "\n",
    "**Implementation Note**: NumPy's [`exp()`](https://numpy.org/doc/stable/reference/generated/numpy.exp.html) function handles vectorized computation efficiently, but requires careful numerical handling to avoid overflow (which you'll implement!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c69ac8-0c00-4827-8921-b6550ed8d5c2",
   "metadata": {},
   "source": [
    "### Logistic Regression: Model\n",
    "<img align=\"left\" src=\"./images/C1_W3_LogisticRegression_right.png\"     style=\" width:300px; padding: 10px; \" > A logistic regression model applies the sigmoid to the familiar linear regression model as shown below:\n",
    "\n",
    "$$ \n",
    "f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b ) \\tag{2} \n",
    "$$ \n",
    "\n",
    "  where\n",
    "\n",
    "  $$\n",
    "  g(z) = \\frac{1}{1+e^{-z}}\\tag{3}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb5760f-12e3-4864-bf3b-cb42ad2f3ca3",
   "metadata": {},
   "source": [
    "### Logistic Regression: Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934cb12b-65aa-4c91-a642-79684d0b58c5",
   "metadata": {},
   "source": [
    "Logistic Regression uses a loss function more suited to the task of categorization where the target is 0 or 1 rather than any number. \n",
    "\n",
    ">**Definition Note:**   In this course, these definitions are used:  \n",
    "**Loss** is a measure of the difference of a single example to its target value while the  \n",
    "**Cost** is a measure of the losses over the training set\n",
    "\n",
    "\n",
    "This is defined: \n",
    "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is:\n",
    "\n",
    "\\begin{equation}\n",
    "  loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = \\begin{cases}\n",
    "    - \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{if $y^{(i)}=1$}\\\\\n",
    "    - \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{if $y^{(i)}=0$}\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value.\n",
    "\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot\\mathbf{x}^{(i)}+b)$ where function $g$ is the sigmoid function.\n",
    "\n",
    "The defining feature of this loss function is the fact that it uses two separate curves. One for the case when the target is zero or ($y=0$) and another for when the target is one ($y=1$). Combined, these curves provide the behavior useful for a loss function, namely, being zero when the prediction matches the target and rapidly increasing in value as the prediction differs from the target. Consider the curves below:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTIyEcDUIGx9kLDohkOyjq8X2OkQZbxLoVW3JyEefVtog&s alt=\"Description of image\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59402019-5be0-4b2f-854a-1eb20e79e6d0",
   "metadata": {},
   "source": [
    "Combined, the curves are similar to the quadratic curve of the squared error loss. Note, the x-axis is $f_{\\mathbf{w},b}$ which is the output of a sigmoid. The sigmoid output is strictly between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeba0f78-881b-4dfd-a2aa-e1ac1aa6b061",
   "metadata": {},
   "source": [
    "The loss function above can be rewritten to be easier to implement.\n",
    "    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)$$\n",
    "  \n",
    "This is a rather formidable-looking equation. It is less daunting when you consider $y^{(i)}$ can have only two values, 0 and 1. One can then consider the equation in two pieces:  \n",
    "when $ y^{(i)} = 0$, the left-hand term is eliminated:\n",
    "$$\n",
    "\\begin{align}\n",
    "loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), 0) &= (-(0) \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - 0\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\\\\n",
    "&= -\\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "and when $ y^{(i)} = 1$, the right-hand term is eliminated:\n",
    "$$\n",
    "\\begin{align}\n",
    "  loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), 1) &=  (-(1) \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - 1\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\\\\\n",
    "  &=  -\\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Why This Loss Function Works:**\n",
    "1. **Probabilistic Foundation**: Derived from maximum likelihood estimation\n",
    "2. **Convex Optimization**: Guarantees unique global minimum (perfect for gradient descent)\n",
    "3. **Penalizes Confidence**: Wrong confident predictions get heavily penalized\n",
    "4. **Smooth Gradients**: Enables stable gradient descent convergence\n",
    "\n",
    "**Connection to Your Implementation**: This loss function is exactly what you'll implement in Task 1, with the addition of L2 regularization to prevent overfitting on the heart disease dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f7e36d-723d-4cdb-a772-412ff188130e",
   "metadata": {},
   "source": [
    "### Logistic Regression: Cost Function (Training Objective)\n",
    "\n",
    "**From Individual Loss to Overall Cost:**\n",
    "\n",
    "The **cost function** aggregates individual losses across all training examples to create our optimization objective:\n",
    "\n",
    "$$ J(\\mathbf{w},b) = \\frac{1}{m} \\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$\n",
    "\n",
    "Where the individual **loss** for each example is:\n",
    "\n",
    "$$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$\n",
    "\n",
    "**The Complete Model Pipeline:**\n",
    "$$\n",
    "\\begin{align}\n",
    "  f_{\\mathbf{w},b}(\\mathbf{x^{(i)}}) &= g(z^{(i)})\\tag{3} \\\\\n",
    "  z^{(i)} &= \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+ b\\tag{4} \\\\\n",
    "  g(z^{(i)}) &= \\frac{1}{1+e^{-z^{(i)}}}\\tag{5} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Optimization Goal**: Find weights $\\mathbf{w}$ and bias $b$ that minimize $J(\\mathbf{w},b)$ using **gradient descent**.\n",
    "\n",
    "\n",
    "## Bridge to Implementation: What You'll Build\n",
    "\n",
    "### Your Assignment Journey:\n",
    "1. **Task 1**: Implement cost function with L2 regularization â†’ **20 points**  \n",
    "2. **Task 2**: Apply to heart disease dataset with preprocessing â†’ **15 points**  \n",
    "3. **Task 3**: Experiment with regularization strength (Î») â†’ **20 points**  \n",
    "4. **Task 4**: Benchmark against scikit-learn LogisticRegression â†’ **20 points**  \n",
    "5. **Task 5**: Explore the `tol` parameter in gradient descentâ€“based models â†’ **15 points**  \n",
    "6. **Task 6**: Summarize key learnings and insights â†’ **5 points** \n",
    "\n",
    "### â­ Bonus Task  \n",
    "Extend your L2-regularized logistic regression by adding a **`tol` stopping criterion**, and compare its behavior with scikit-learnâ€™s implementation -> **15 points**\n",
    "   \n",
    "### Key Implementation Challenges You'll Solve:\n",
    "\n",
    "- **Numerical Stability**: Prevent `log(0)` errors that break gradient descent\n",
    "- **Vectorization**: Implement efficient matrix operations for real-world performance\n",
    "- **Regularization**: Add L2 penalty to prevent overfitting on complex medical data\n",
    "- **Gradient Computation**: Derive and implement the gradient for weight updates\n",
    "- **Convergence Monitoring**: Track training progress and detect overfitting\n",
    "\n",
    "### Real-World Impact:\n",
    "Your implementation will predict heart disease risk using patient dataâ€”a model that could potentially assist medical professionals in making life-saving decisions. The mathematical rigor you develop here translates directly to production ML systems used in healthcare, finance, and technology.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab70e382-25b8-470c-8aea-c4fbb10856c4",
   "metadata": {},
   "source": [
    "## Task 1: Implementing L2 Regularized Logistic Regression with Vectorized Gradient Descent **(20 points)**\n",
    "\n",
    "###  **Objective**\n",
    "Implement a complete L2 regularized (Ridge) logistic regression algorithm from scratch using vectorized operations to prevent overfitting in binary classification problems.\n",
    "\n",
    "### ðŸ“‹ **Implementation Requirements**\n",
    "\n",
    "#### **Main Function Signature**\n",
    "Your implementation should create a main training function that takes these inputs:\n",
    "\n",
    "- **X_train** (ndarray): Training feature matrix (m Ã— n) - **Note**: Should include bias column of ones as first column\n",
    "- **y_train** (ndarray): Training labels (m Ã— 1) \n",
    "- **X_test** (ndarray): Test feature matrix for monitoring overfitting - **Note**: Should include bias column  \n",
    "- **y_test** (ndarray): Test labels for performance tracking\n",
    "- **w_in** (ndarray): Initial weights ((n+1) Ã— 1) - **Includes bias weight as first element**\n",
    "- **alpha** (float): Learning rate (typically 0.001 - 0.01)\n",
    "- **num_iters** (int): Number of training iterations\n",
    "- **lambda_reg** (float): L2 regularization strength (Î» â‰¥ 0)\n",
    "\n",
    "#### **Expected Outputs**\n",
    "- **Optimized parameters**: Final weights after training\n",
    "- **Training cost history**: Cost values at each iteration on training data\n",
    "- **Test cost history**: Cost values at each iteration on test data (for overfitting analysis)\n",
    "\n",
    "### **Implementation Structure**\n",
    "\n",
    "#### **Step 1: Cost Function with L2 Regularization**\n",
    "```python\n",
    "def compute_cost_logistic_ridge(X, y, w, lambda_reg):\n",
    "```\n",
    "**Mathematical Formula:**\n",
    "$$J(w) = \\frac{1}{m}\\sum_{i=1}^{m} \\left[ -y^{(i)} \\log(h_w(x^{(i)})) - (1-y^{(i)}) \\log(1-h_w(x^{(i)})) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} w_j^2$$\n",
    "\n",
    "Where $h_w(x) = \\sigma(w^T x)$ and $\\sigma(z) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "**Key Implementation Notes:**\n",
    "\n",
    "- Use vectorized operations for efficiency\n",
    "- **Bias handling**: Don't regularize the bias term (w[0]) - exclude from L2 penalty\n",
    "- **Data preparation**: Add column of ones to X matrices for bias term\n",
    "- Handle numerical stability with log functions\n",
    "\n",
    "**Why Numerical Stability Matters:**\n",
    "\n",
    "- **When h = 0**: `log(0) = -âˆž` â†’ **NaN in cost calculations**\n",
    "- **When h = 1**: `log(1-h) = log(0) = -âˆž` â†’ **NaN in cost calculations**  \n",
    "- **When it occurs**: Sigmoid outputs extreme values (overconfident predictions)\n",
    "- **Consequence**: Entire gradient descent breaks due to NaN propagation\n",
    "\n",
    "   **Practical Example:**\n",
    "   ```python\n",
    "   # Without clipping - BREAKS!\n",
    "   h = np.array([0.0, 0.5, 1.0])  # Sigmoid predictions\n",
    "   cost = -np.log(h)  # Returns: [inf, 0.693, 0.0]\n",
    "   # â†‘ inf values propagate to gradients â†’ NaN â†’ training failure\n",
    "\n",
    "   # With clipping - WORKS!\n",
    "   h_safe = np.clip(h, 1e-15, 1-1e-15)  # Safe range: [1e-15, 1-1e-15]\n",
    "   cost = -np.log(h_safe)  # Returns: [34.54, 0.693, 34.54]  \n",
    "   # â†‘ All finite values â†’ stable gradients â†’ successful training\n",
    "   ```\n",
    "\n",
    "**Professional Tip**: All major ML libraries (TensorFlow, PyTorch, sklearn) use similar clipping internally!\n",
    "\n",
    "#### **Step 2: Gradient Computation with L2 Regularization**\n",
    "```python\n",
    "def compute_gradient_logistic_ridge(X, y, w, lambda_reg):\n",
    "```\n",
    "**Mathematical Formula:**\n",
    "$$\\frac{\\partial J}{\\partial w_j} = \\frac{1}{m} X^T (\\sigma(Xw) - y) + \\frac{\\lambda}{m} w_j \\quad \\text{(for } j > 0\\text{)}$$\n",
    "$$\\frac{\\partial J}{\\partial w_0} = \\frac{1}{m} X^T (\\sigma(Xw) - y) \\quad \\text{(bias term, no regularization)}$$\n",
    "\n",
    "**Implementation Note**: The gradient vector should be same shape as w_in ((n+1) Ã— 1), where the first element corresponds to the bias term gradient (no regularization) and remaining elements include the L2 regularization term.\n",
    "\n",
    "#### **Step 3: Gradient Descent Algorithm**\n",
    "```python\n",
    "def gradient_descent_logistic_ridge(X_train, y_train, X_test, y_test, w_in, alpha, num_iters, lambda_reg):\n",
    "```\n",
    "\n",
    "#### ðŸ’¡ **Implementation Tips**\n",
    "\n",
    "1. **Bias Term Setup**: \n",
    "   - Add column of ones as **first column** of X matrices: `X = np.column_stack([np.ones(m), X_features])`\n",
    "   - Initialize w_in with shape (n+1, 1) where w[0] is bias, w[1:] are feature weights\n",
    "2. **Vectorization**: Use matrix operations instead of loops for efficiency\n",
    "3. **Numerical Stability**: \n",
    "   - Clip extreme values in sigmoid to prevent overflow\n",
    "   - Use `np.clip(predictions, 1e-15, 1-1e-15)` for log calculations\n",
    "4. **Regularization Implementation**: \n",
    "   - Create regularization vector: `reg_term = np.copy(w); reg_term[0] = 0` (exclude bias)\n",
    "   - Add to gradient: `gradient + (lambda_reg/m) * reg_term`\n",
    "5. **Monitoring**: Track both training and test costs to observe overfitting\n",
    "\n",
    "\n",
    "#### âœ… **Validation Checklist**\n",
    "- [ ] Cost decreases monotonically during training\n",
    "- [ ] Implementation uses vectorized operations (no explicit loops over samples)\n",
    "- [ ] Regularization term correctly excludes bias\n",
    "- [ ] Both training and test costs are tracked\n",
    "- [ ] Function handles edge cases (very small/large predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1925ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f788436e-7f8f-46c4-9d0d-d1f629921869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_logistic_ridge(X,y,w,lambda_reg):\n",
    "    m = len(y)\n",
    "    h = np.clip(sigmoid(X@w), 1e-15, 1 - 1e-15)\n",
    "    sum_cost = -((y @ np.log(h)) + ((1-y) @ np.log(1-h))) / m\n",
    "    l2_penalty = (lambda_reg / (2*m)) * np.sum(w[1:] ** 2)\n",
    "    cost = sum_cost + l2_penalty\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0077e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_logistic_ridge(X, y, w, lambda_reg):\n",
    "    m = len(y)\n",
    "    dj_dw = (X.T@(sigmoid(X@w) - y))/m\n",
    "    dj_dw[1:] += lambda_reg * w[1:]\n",
    "    return dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80a2c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_logistic_ridge(X_train, y_train, X_test, y_test, w_in, alpha, num_iters, lambda_reg):\n",
    "    w = w_in.copy()\n",
    "    J_history = []\n",
    "    J_test_history = []\n",
    "    for i in range(num_iters):\n",
    "        dj_dw = compute_gradient_logistic_ridge(X_train, y_train, w, lambda_reg)\n",
    "        w = w - alpha * dj_dw\n",
    "        cost = compute_cost_logistic_ridge(X_train, y_train, w, lambda_reg)\n",
    "        test_cost = compute_cost_logistic_ridge(X_test, y_test, w, lambda_reg)\n",
    "        J_history.append(cost)\n",
    "        J_test_history.append(test_cost)\n",
    "        if i % 5000 == 0 or i == num_iters - 1:\n",
    "            print(f\"Iteration {i}: Training Cost {cost}, Test Cost {test_cost}\")\n",
    "    return w, J_history, J_test_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa3f1d8-5663-4115-aa84-a241a059611e",
   "metadata": {},
   "source": [
    "\n",
    "## Task 2: Apply your implementation on a real dataset **(15 points)**\n",
    "\n",
    "### **Objective**\n",
    "Apply your L2 regularized logistic regression implementation to predict heart disease using a real medical dataset, demonstrating proper data preprocessing, model training, and performance visualization.\n",
    "\n",
    "### **Dataset Overview** \n",
    "The **heart_disease_classification.csv** contains medical data for heart disease prediction with:\n",
    "\n",
    "- **Target**: Binary classification (0=No Disease, 1=Disease)  \n",
    "- **Features**: Mix of numerical (age, cholesterol) and categorical (chest pain type, slope)\n",
    "- **Challenge**: Categorical variables need proper encoding for your algorithm\n",
    "\n",
    "### ðŸ“‹ **Step-by-Step Implementation Guide**\n",
    "\n",
    "#### **Step 1: Data Loading and Initial Exploration**\n",
    "\n",
    "In this step, you will begin by **importing the dataset into your working environment** (e.g., using `pandas.read_csv()` for CSV files). After successfully loading the data, perform some **basic exploratory checks** to understand its structure and contents. This includes:\n",
    "\n",
    "- Displaying the **first few rows** of the dataset with `.head()` to get a sense of the data format  \n",
    "- Checking the **dimensions** of the dataset with `.shape` to see how many rows and columns are available  \n",
    "- Inspecting the **column names and data types** with `.info()` to identify numerical, categorical, and datetime features  \n",
    "- Reviewing **summary statistics** with `.describe()` to quickly understand ranges, averages, and distributions of numeric columns  \n",
    "\n",
    "This step ensures that you have properly loaded the dataset and gained a **high-level understanding of its characteristics**, which is essential before moving into deeper transformation, or modeling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74af2159",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease = pd.read_csv('heart_disease_classification.csv')\n",
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f041da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape:\", heart_disease.shape)\n",
    "print(\"info:\", heart_disease.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08e3bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"summary:\", heart_disease.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b49957",
   "metadata": {},
   "source": [
    "#### **Step 2: Data Shuffling and Feature-Target Separation (*random_state=0*)**\n",
    "\n",
    "After loading and exploring the dataset, the next step is to **prepare it for modeling**. This involves two tasks:\n",
    "\n",
    "1. **Shuffling the data**\n",
    "   - Many real-world datasets, especially in domains like **medicine**, are collected in sequential order (e.g., sorted by collection date, patient ID, or clinical visit number).  \n",
    "   - If you split the dataset without shuffling, the training set may contain only earlier cases and the test set may contain only later ones. This can introduce **temporal or ID-based bias** and reduce the modelâ€™s ability to generalize.  \n",
    "   - Shuffling ensures that the **distribution of samples is random**, giving both training and test sets a representative mix of patients and outcomes.  \n",
    "   - We use `random_state=0` to make the shuffle **reproducible**, so everyone gets the same randomized order.  \n",
    "\n",
    "2. **Separating features and target**\n",
    "   - Define the **target variable** (the outcome you want to predict, e.g., disease status, biomarker level, or treatment response).  \n",
    "   - Select the **features** (independent variables, such as patient demographics, lab values, or imaging-derived metrics).  \n",
    "   - Split the dataset into:  \n",
    "     - `X` â†’ the **feature matrix** (all input variables except the target)  \n",
    "     - `y` â†’ the **target vector** (the label or response variable)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a557e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_heart_disease = heart_disease.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "X = shuffled_heart_disease.drop('target', axis=1)\n",
    "y = shuffled_heart_disease[['target']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3de32af",
   "metadata": {},
   "source": [
    "#### **Step 3: Trainâ€“Test Split (random_state=42) and Feature Scaling**\n",
    "\n",
    "**RUNNING THIS AFTER PD GET DUMMIES OTHERWISE INTERACTS**\n",
    "\n",
    "Once the dataset is shuffled and the features (`X`) and target (`y`) are separated, the next step is to **divide the data into training and testing subsets**.  \n",
    "\n",
    "- **Training set**: Used to fit the model (learn the patterns from the data).  \n",
    "- **Test set**: Held back to evaluate how well the model generalizes to unseen data.  \n",
    "\n",
    "We typically use **80% of the data for training** and **20% for testing**, though this ratio can be adjusted based on dataset size and problem context.  \n",
    "\n",
    "To ensure reproducibility, we specify `random_state=42` when performing the split.  \n",
    "\n",
    "##### âœ… Why split the data?\n",
    "\n",
    "- Prevents **overfitting** by ensuring the model is evaluated on unseen data.  \n",
    "- Mimics a real-world scenario where the model encounters new patient cases.  \n",
    "- Provides an unbiased estimate of model performance. \n",
    "\n",
    "##### âœ… Why scaling matters\n",
    "- Features may be measured on very different scales (e.g., **age** ranges 20â€“80, while **cholesterol** may range 100â€“600).  \n",
    "- Algorithms that use **gradient descent** (e.g., logistic regression, neural networks) can converge slowly or get stuck if features are not scaled properly.  \n",
    "- Scaling puts all features on a **comparable range**, improving both **training stability** and **model performance**. \n",
    "\n",
    "**Important Notes**:  \n",
    "\n",
    "- Use `fit_transform()` **only on the training set** so the scaler does not \"see\" information from the test set (**avoiding data leakage**).  \n",
    "- Apply the fitted scaler to the **test set** using `transform()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea75899",
   "metadata": {},
   "source": [
    "#### **Step 4: Categorical Variable Encoding**\n",
    "\n",
    "The dataset contains **categorical variables** that must be converted into numerical form before modeling. Most machine learning algorithms cannot directly handle categorical text values, so we use **binary (dummy) encoding**.\n",
    "\n",
    "\n",
    "**Required Encoding for These Columns:**\n",
    "- **`cp`** (Chest Pain Type): Convert into binary dummy variables  \n",
    "- **`thal`** (Thalassemia): Convert into binary dummy variables  \n",
    "- **`slope`** (ST Slope): Convert into binary dummy variables  \n",
    "\n",
    "\n",
    "##### âœ… Why Encode?\n",
    "- Machine learning algorithms work with **numbers**, not text labels.  \n",
    "- Encoding categorical variables as dummy variables prevents the model from assuming an **ordinal relationship** between categories (e.g., category \"3\" is not â€œgreater thanâ€ category \"1\").  \n",
    "- This ensures that the algorithm treats categories as **independent groups**.  \n",
    "\n",
    "\n",
    "\n",
    "##### Example (using `pandas.get_dummies`)\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Encode categorical variables into dummy variables\n",
    "df_encoded = pd.get_dummies(df, columns=['cp', 'thal', 'slope'], drop_first=True)\n",
    "\n",
    "# View the new columns\n",
    "df_encoded.head()\n",
    "```\n",
    "Here:\n",
    "\n",
    "- `pd.get_dummies()` automatically creates binary (0/1) columns for each category.\n",
    "\n",
    "- `drop_first=True` avoids multicollinearity by dropping one category from each variable as the reference group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc8ad38",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X, columns = [\"cp\", \"thal\", \"slope\"], drop_first = True)   \n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060a8583",
   "metadata": {},
   "outputs": [],
   "source": [
    "##RUNNING THIS AFTER PD.GET DUMMIES\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93c25c1",
   "metadata": {},
   "source": [
    "#### **Step 5: Model Training and Hyperparameter Selection**\n",
    "\n",
    "With the training and test sets prepared, the next step is to **train the model**. This involves selecting appropriate **hyperparameters** that control how the algorithm learns.  \n",
    "\n",
    "\n",
    "##### âœ… Key Considerations\n",
    "- **Learning Rate (`Î±`)**: Controls the step size during gradient descent updates.  \n",
    "  - Too large â†’ the algorithm may diverge.  \n",
    "  - Too small â†’ convergence becomes very slow.  \n",
    "\n",
    "- **Number of Iterations / Epochs**: Determines how many times the algorithm goes through the training data.  \n",
    "  - Choose enough iterations for convergence, but avoid unnecessary computation.  \n",
    "\n",
    "- **Regularization (if applicable)**: Prevents overfitting by penalizing large weights.  \n",
    "\n",
    "#####  Practical Setup\n",
    "- Initialize parameters (`w_in`).  \n",
    "- Select a reasonable learning rate (`alpha`).  \n",
    "- Choose the number of iterations (`num_iters`).  \n",
    "- Set a regularization strength (`lambda_reg`).  \n",
    "\n",
    "As long as the model **converges stably**, your hyperparameters are acceptable.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506127c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.column_stack((np.ones(len(X_train)), X_train))\n",
    "X_test = np.column_stack((np.ones(len(X_test)), X_test))\n",
    "w_in = np.zeros(X_train.shape[1])\n",
    "alpha = 0.01\n",
    "num_iters = 50000\n",
    "lambda_reg = 0.01\n",
    "\n",
    "w_final, J_history, J_test_history = gradient_descent_logistic_ridge(X_train, y_train.values.flatten(), X_test, y_test.values.flatten(), w_in, alpha, num_iters, lambda_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec5e03c",
   "metadata": {},
   "source": [
    "#### **Step 6: Plot the Learning Curve of Gradient Descent**\n",
    "\n",
    "After training the model with gradient descent, it is important to **visualize the learning process**. A learning curve shows how the modelâ€™s **loss or accuracy changes** over iterations (epochs) for both the **training set** and the **test set**.\n",
    "\n",
    "\n",
    "##### âœ… Why Learning Curves Matter\n",
    "- **Convergence Check**: Helps verify whether gradient descent is converging to a minimum.  \n",
    "- **Underfitting vs. Overfitting**:  \n",
    "  - If both training and test errors are high â†’ underfitting.  \n",
    "  - If training error is low but test error is high â†’ overfitting.  \n",
    "- **Hyperparameter Tuning**: Reveals whether changes in learning rate (`Î±`), number of iterations, or regularization improve training stability.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2c82e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting the learning curve to show the overfitting issue of the model\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[0].plot(J_history, label='Training')\n",
    "axs[0].plot(J_test_history, label='Test')\n",
    "axs[0].set_xlabel('Iterations')\n",
    "axs[0].set_ylabel('Cost')\n",
    "axs[0].set_title('Cost vs. Iterations')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(J_history[15000:], label='Training')\n",
    "axs[1].plot(J_test_history[15000:], label='Test')\n",
    "axs[1].set_xlabel('Iterations')\n",
    "axs[1].set_ylabel('Cost')\n",
    "axs[1].set_title('Cost vs. Iterations after 5000 iterations')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bf8091-bab2-4738-aad7-589594f261a9",
   "metadata": {},
   "source": [
    "## Task 3: Regularization Analysis - Finding the Optimal Î» Value **(20 points)**\n",
    "\n",
    "`lambda_reg` controls the strength of the regularization applied to the model.  \n",
    "- When `lambda_reg` is set to **zero**, regularization is effectively turned off.  \n",
    "- As `lambda_reg` increases, the penalty for large weights becomes more significant, helping to **reduce overfitting**.  \n",
    "\n",
    "\n",
    "\n",
    "### ðŸ”§ Task Instructions\n",
    "- Experiment with different values of `lambda_reg` in the set **[0.0, 0.01, 0.03, 0.1, 0.3]**.  \n",
    "- Plot the **learning curves** for both the training and test sets on the same figure to visualize the impact of each value.  \n",
    "- Determine which value of `lambda_reg` yields the **best performance** on this dataset.  \n",
    "- Report the performance (accuracy, precision, recall) using the best value of:  \n",
    "  - **Accuracy**  \n",
    "  - **Precision**  \n",
    "  - **Recall**  \n",
    "\n",
    "\n",
    "\n",
    "**Hyperparameters for this task**:  \n",
    "- `learning_rate = 0.005`  \n",
    "- `num_iterations = 1200`  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498c762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "results = {}\n",
    "cost_histories = {}\n",
    "\n",
    "for i in [0.0, 0.01, 0.03, 0.1, 0.3]:\n",
    "    lambda_reg = i\n",
    "    w_in = np.zeros(X_train.shape[1])\n",
    "    num_iters = 1200\n",
    "    alpha = .005\n",
    "    w_final, J_history, J_test_history = gradient_descent_logistic_ridge(\n",
    "        X_train, y_train.values.flatten(), X_test, y_test.values.flatten(),\n",
    "        w_in, alpha, num_iters, lambda_reg\n",
    "    )\n",
    "    y_pred_prob = sigmoid(X_test@w_final)\n",
    "    y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    results[i] = {'accuracy': acc, 'precision': prec, 'recall': rec}\n",
    "    cost_histories[i] = (J_history, J_test_history)\n",
    "\n",
    "# Plot all learning curves together\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in cost_histories:\n",
    "    plt.plot(cost_histories[i][0], label=f'Train Î»={i}')\n",
    "    plt.plot(cost_histories[i][1], label=f'Test Î»={i}', linestyle='--')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost vs Iterations for Different Î»')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107a6932",
   "metadata": {},
   "source": [
    "Based on the cost plots, it appears as though lambda that has the least cost is where there is no regularization, i.e. lambda = 0.0. This lambda is associated with an accuracy of 0.803, precision of 0.811, and recall of .857."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5f362b-2348-4a83-9b6c-545a14ab7a3a",
   "metadata": {},
   "source": [
    "## Task 4: Custom vs. scikit-learn â€” Comprehensive Benchmark Analysis **(20 points)**\n",
    "\n",
    "- Use the **`LogisticRegression`** model from **scikit-learn** to re-evaluate the dataset while keeping the same (or as similar as possible) hyperparameter settings for a **fair comparison**.  \n",
    "- Report the performance using the **same evaluation metrics** as previously used.  \n",
    "- Compare the results to your **custom implementation**. Analyze whether scikit-learnâ€™s built-in logistic regression achieves similar, better, or worse performance.  \n",
    "- Explain the potential reasons for any differences you observe. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f11d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(penalty='l2', C=1/0.005, solver='liblinear', max_iter=1200)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "print(f\"Sklearn Logistic Regression - Accuracy: {acc}, Precision: {prec}, Recall: {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a958662",
   "metadata": {},
   "source": [
    "In this case, the accuracy and precision are both lower, but the recall is higher than our custom l2 model. This suggests to me that this model is identifying more of the true positive cases, but doing so very liberally as it is just applying more positive classifications in general, which in the process is falsely identifying them hence why precision and accuracy are down. This may be because of the closed-form solution for this, which may simplify an otherwise complex and uneven landscape and miss many of these unique cases that our customized gradient descent function can catch correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f2c627-aaa2-4dc0-a68c-dd4dea175ae5",
   "metadata": {},
   "source": [
    "## Task 5: Understanding Convergence - The `tol` Parameter Deep Dive **(15 points)**\n",
    "\n",
    "In many models that use gradient descentâ€“based optimization, `tol` is a key hyperparameter.  \n",
    "In this task, you will explore its role using **logistic regression**.\n",
    "\n",
    "### **Understanding the Tolerance Parameter**\n",
    "The `tol` parameter in sklearn's LogisticRegression controls **when to stop the optimization process**:\n",
    "\n",
    "- **What it measures**: Maximum change in the optimization objective between iterations\n",
    "- **Stopping criterion**: Training stops when improvement falls below this threshold\n",
    "- **Trade-off**: Tighter tolerance (smaller values) â†’ more iterations â†’ potentially better convergence\n",
    "- **Efficiency**: Looser tolerance (larger values) â†’ fewer iterations â†’ faster training but potentially suboptimal results\n",
    "\n",
    "### **Mathematical Context**\n",
    "For solver convergence, sklearn typically uses:\n",
    "$$|\\nabla J(\\mathbf{w}^{(t)}) - \\nabla J(\\mathbf{w}^{(t-1)})| < \\text{tol}$$\n",
    "Or for objective function change:\n",
    "$$|J(\\mathbf{w}^{(t)}) - J(\\mathbf{w}^{(t-1)})| < \\text{tol}$$\n",
    "\n",
    "\n",
    "### **Expected Insights**\n",
    "\n",
    "Experiment with the following `tol` values to explore their impact on model convergence, training speed, and performance:\n",
    "\n",
    "**Typical Patterns You Should Observe:**\n",
    "- **Tighter tolerance (`1e-6`, `1e-7`)** â†’ Training is slower but may yield slightly better performance.  \n",
    "- **Moderate tolerance (`1e-4`, `1e-5`)** â†’ A good balance between speed and accuracy (close to scikit-learnâ€™s default).  \n",
    "- **Loose tolerance (`1e-1`, `1e-2`)** â†’ Faster training but may stop too early, leading to suboptimal results.  \n",
    "- **Performance plateau** â†’ Beyond a certain level of tightness, further decreasing tolerance does not yield meaningful gains.  \n",
    "- **Convergence issues** â†’ Extremely tight tolerance may require more iterations than allowed, preventing proper convergence.  \n",
    "\n",
    "\n",
    "### âœ… **Success Criteria**\n",
    "- [ ] Systematic testing across tolerance range (8+ values)\n",
    "- [ ] Comprehensive performance and efficiency metrics\n",
    "- [ ] Professional visualization with multiple perspectives\n",
    "- [ ] Clear identification of trade-offs and recommendations\n",
    "- [ ] Evidence-based analysis of sklearn's default choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc93239c-180f-49fa-b77d-b1ece150fdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tight tolerance\n",
    "tol = 1e-7\n",
    "model = LogisticRegression(penalty='l2',C=1/0.005,max_iter=50000,tol=tol)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "print(f\"Sklearn Logistic Regression with tight tol - Accuracy: {acc}, Precision: {prec}, Recall: {rec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2985b4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#moderate tolerance\n",
    "tol = 1e-4\n",
    "model = LogisticRegression(penalty='l2',C=1/0.005,max_iter=50000,tol=tol)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "print(f\"Sklearn Logistic Regression with moderate tol - Accuracy: {acc}, Precision: {prec}, Recall: {rec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa9741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loose tolerance\n",
    "tol = 1e-1\n",
    "model = LogisticRegression(penalty='l2',C=1/0.005,max_iter=50000,tol=tol)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "print(f\"Sklearn Logistic Regression with loose tol - Accuracy: {acc}, Precision: {prec}, Recall: {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7a8991",
   "metadata": {},
   "source": [
    "## Bonus Task: Implementing `tol` in Your L2 Gradient Descent (15 points)\n",
    "\n",
    "*This is a bonus task, feel free to skip this task*\n",
    "\n",
    "To make your **custom L2-regularized gradient descent** implementation comparable with scikit-learnâ€™s `LogisticRegression`, you should add the `tol` parameter as a **stopping criterion**.  \n",
    "\n",
    "### âœ… Instructions:\n",
    "1. During each iteration of gradient descent, compute the **change in cost (loss)** between the current and previous step.  \n",
    "2. If the change is **smaller than `tol`**, stop the training loop early.  \n",
    "3. Use the same tolerance values (`tol`) as in scikit-learn for a fair comparison.  \n",
    "4. Report:  \n",
    "   - The number of iterations completed before convergence.  \n",
    "   - Final training and test performance metrics.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc28ad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_logistic_ridge(X_train, y_train, X_test, y_test, w_in, alpha, num_iters, lambda_reg, tol):\n",
    "    w = w_in.copy()\n",
    "    J_history = []\n",
    "    J_test_history = []\n",
    "    for i in range(num_iters):\n",
    "        dj_dw = compute_gradient_logistic_ridge(X_train, y_train, w, lambda_reg)\n",
    "        w = w - alpha * dj_dw\n",
    "        cost = compute_cost_logistic_ridge(X_train, y_train, w, lambda_reg)\n",
    "        test_cost = compute_cost_logistic_ridge(X_test, y_test, w, lambda_reg)\n",
    "        J_history.append(cost)\n",
    "        J_test_history.append(test_cost)\n",
    "        if i > 0 and abs(J_history[-1] - J_history[-2]) < tol:\n",
    "            print(f\"Converged at iteration {i}: Training Cost {cost}, Test Cost {test_cost}\")\n",
    "            break\n",
    "        if i % 5000 == 0 or i == num_iters - 1:\n",
    "            print(f\"Iteration {i}: Training Cost {cost}, Test Cost {test_cost}\")\n",
    "    return w, J_history, J_test_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d92a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_in = np.zeros(X_train.shape[1])\n",
    "alpha = 0.01\n",
    "num_iters = 50000\n",
    "lambda_reg = 0.01\n",
    "tol = 1e-5\n",
    "w_final, J_history, J_test_history = gradient_descent_logistic_ridge(X_train, y_train.values.flatten(), X_test, y_test.values.flatten(), w_in, alpha, num_iters, lambda_reg, tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d9bb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = sigmoid(X_test@w_final)\n",
    "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "results[i] = {'accuracy': acc, 'precision': prec, 'recall': rec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227931d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Custom Logistic Regression with L2 with tol - Accuracy: {acc}, Precision: {prec}, Recall: {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baf1dfd",
   "metadata": {},
   "source": [
    "## Task 6: Summarize your findings below **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eef6d4",
   "metadata": {},
   "source": [
    "I found once again that the custom l2 logistic regression implementation had a much better performance than sklearn, especially in precision and accuracy, likely due to the more customized navigation of the function's landscape.\n",
    "\n",
    "Tol is an interesting hyperparameter that can be utilized to adjust for how much the optimization trades-off efficiency vs. accuracy. Depending on your purpose, you can utilize a tighter tolerance that will favor better accuracy whereas a looser tolerance will favor efficiency more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180cfbf3",
   "metadata": {},
   "source": [
    "## AI Use Disclosure **(5 pts)**\n",
    "\n",
    "In **1â€“3 short paragraphs**, clearly state whether **generative AI tools** were used to complete any part of this assignment.\n",
    "\n",
    "- If **no AI was used**, write:  \n",
    "  *â€œI did not use generative AI tools on this assignment.â€*  \n",
    "\n",
    "- If **AI was used**, you must specify:  \n",
    "  1. **Tool(s)** used (e.g., ChatGPT, GitHub Copilot, Claude, etc.)  \n",
    "  2. **How** they were used (e.g., idea generation, debugging, code suggestions, writing help)  \n",
    "  3. **Where** they influenced your work (which questions/sections)  \n",
    "  4. **Edits & verification** you made (how you checked correctness, what you modified)  \n",
    "\n",
    "### AI Use Template\n",
    "- **Used AI?** Yes / No  \n",
    "- **Tool(s):**  \n",
    "- **Purpose / How used (1â€“3 sentences):**  \n",
    "- **Scope (which questions/sections):**  \n",
    "- **Edits & verification (what you changed and how you checked correctness):**  \n",
    "\n",
    "> **Example (for illustration only):**  \n",
    "> - Used AI? Yes  \n",
    "> - Tools: ChatGPT (free), GitHub Copilot  \n",
    "> - How used: Asked for a pandas snippet to impute missing values and for a seaborn example.  \n",
    "> - Scope: Q1(b) imputation, Q2(a) first draft of bar chart code.  \n",
    "> - Edits & verification: Rewrote code to use `groupby().transform('median')`; validated results by comparing summary stats; added axis labels manually.  \n",
    "\n",
    "**Grading (5 points):**  \n",
    "- **5 points** = complete, specific, and honest (tools named, usage described clearly)  \n",
    "- **3â€“4 points** = vague or missing some details  \n",
    "- **1â€“2 points** = minimal effort or very unclear  \n",
    "- **0 points** = missing or misleading disclosure  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01b68f0",
   "metadata": {},
   "source": [
    "I used GitHub Copilot to help complete this assignment in order to resolve any syntax errors/other errors I was running into while writing my code.\n",
    "\n",
    "In particular, I had been running into issues in the formation of the gradient_descent_logistic_ridge (TASK 1) function due to misinterpretation based on the instructions, which was trying to compute a dot product of two mismatched matrix shapes. However, after using github copilot to help me resolve this issue, I was able to identify that one of my earlier defined functions was leading to this error. \n",
    "\n",
    "I also utilized copilot to help me create the plot for TASK 3 where I was trying to find the optimal lambda in which instead of creating multiple separate plots that was hard to explicitly compare, I plotted all the cost curves on one to clearly identify which was outperforming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f6e28f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
